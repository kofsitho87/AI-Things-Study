{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Document Agents (V1)\n",
    "이 가이드에서는 LlamaIndex 문서를 통해 다중 문서 에이전트를 설정하는 방법을 알아봅니다.\n",
    "\n",
    "이는 다음과 같은 추가 기능을 갖춘 V0 다중 문서 에이전트의 확장입니다.\n",
    "- 문서(도구) 검색 중 순위 재지정\n",
    "- 에이전트가 계획을 세우는 데 사용할 수 있는 쿼리 계획 도구\n",
    "\n",
    "우리는 다음 아키텍처를 사용하여 이를 수행합니다.\n",
    "- 각 문서에 대해 \"문서 에이전트\"를 설정합니다. 각 문서 에이전트는 해당 문서 내에서 QA/요약을 수행할 수 있습니다.\n",
    "- 이 문서 에이전트 집합에 대해 최상위 에이전트를 설정합니다. 도구 검색을 수행한 다음 도구 세트에 대해 CoT를 수행하여 질문에 답합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 설정 및 다운로드\n",
    "이 섹션에서는 LlamaIndex 문서를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"docs.llamaindex.ai\"\n",
    "docs_url = \"https://docs.llamaindex.ai/en/latest/\"\n",
    "!wget -e robots=off --recursive --no-clobber --page-requisites --html-extension --convert-links --restrict-file-names=windows --domains {domain} --no-parent {docs_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/heewungsong/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load HTML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1197\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "all_files_gen = Path(\"./docs.llamaindex.ai/\").rglob(\"*\")\n",
    "all_files = [f.resolve() for f in all_files_gen]\n",
    "\n",
    "all_html_files = [f for f in all_files if f.suffix.lower() == \".html\"]\n",
    "\n",
    "print(len(all_html_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/index.html'),\n",
       " PosixPath('/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/index.html'),\n",
       " PosixPath('/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/using_llms/using_llms/index.html'),\n",
       " PosixPath('/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/using_llms/privacy/index.html'),\n",
       " PosixPath('/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/loading/llamahub/index.html')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_html_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx 0/1197\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/index.html\n",
      "Idx 1/1197\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/index.html\n",
      "Idx 2/1197\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/using_llms/using_llms/index.html\n",
      "Idx 3/1197\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/using_llms/privacy/index.html\n",
      "Idx 4/1197\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/loading/llamahub/index.html\n",
      "Idx 5/1197\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/loading/loading/index.html\n",
      "Idx 6/1197\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/evaluating/evaluating/index.html\n",
      "Idx 7/1197\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/evaluating/cost_analysis/index.html\n",
      "Idx 8/1197\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/evaluating/cost_analysis/usage_pattern/index.html\n",
      "Idx 9/1197\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/storing/storing/index.html\n",
      "Idx 10/1197\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/tracing_and_debugging/tracing_and_debugging/index.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.readers.file import UnstructuredReader\n",
    "\n",
    "reader = UnstructuredReader()\n",
    "\n",
    "# TODO: set to higher value if you want more docs\n",
    "doc_limit = 10\n",
    "\n",
    "docs = []\n",
    "for idx, f in enumerate(all_html_files):\n",
    "    if idx > doc_limit:\n",
    "        break\n",
    "    print(f\"Idx {idx}/{len(all_html_files)}\")\n",
    "    loaded_docs = reader.load_data(file=f, split_documents=True)\n",
    "    # Hardcoded Index. Everything before this is ToC for all pages\n",
    "    start_idx = 72\n",
    "    loaded_doc = Document(\n",
    "        text=\"\\n\\n\".join([d.get_content() for d in loaded_docs[72:]]),\n",
    "        metadata={\"path\": str(f)},\n",
    "    )\n",
    "    print(loaded_doc.metadata[\"path\"])\n",
    "    docs.append(loaded_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/index.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/index.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/using_llms/using_llms/index.html\n",
      "For example, if you have Ollama installed and running:\n",
      "\n",
      "from\n",
      "\n",
      "llama_index.llms.ollama\n",
      "\n",
      "import\n",
      "\n",
      "Ollama\n",
      "\n",
      "from\n",
      "\n",
      "llama_index.core\n",
      "\n",
      "import\n",
      "\n",
      "Settings\n",
      "\n",
      "Settings\n",
      "\n",
      "llm\n",
      "\n",
      "Ollama\n",
      "\n",
      "model\n",
      "\n",
      "\"llama2\"\n",
      "\n",
      "request_timeout\n",
      "\n",
      "60.0\n",
      "\n",
      "See the custom LLM's How-To for more details.\n",
      "\n",
      "Prompts#\n",
      "\n",
      "By default LlamaIndex comes with a great set of built-in, battle-tested prompts that handle the tricky work of getting a specific LLM to correctly handle and format data. This is one of the biggest benefits of using LlamaIndex. If you want to, you can customize the prompts\n",
      "\n",
      "\n",
      "\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/using_llms/privacy/index.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/loading/llamahub/index.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/loading/loading/index.html\n",
      "getenv\n",
      "\n",
      "\"DB_NAME\"\n",
      "\n",
      "),\n",
      "\n",
      "query\n",
      "\n",
      "\"SELECT * FROM users\"\n",
      "\n",
      "documents\n",
      "\n",
      "reader\n",
      "\n",
      "load_data\n",
      "\n",
      "query\n",
      "\n",
      "query\n",
      "\n",
      "There are hundreds of connectors to use on LlamaHub!\n",
      "\n",
      "Creating Documents directly#\n",
      "\n",
      "Instead of using a loader, you can also use a Document directly.\n",
      "\n",
      "from\n",
      "\n",
      "llama_index.core\n",
      "\n",
      "import\n",
      "\n",
      "Document\n",
      "\n",
      "doc\n",
      "\n",
      "Document\n",
      "\n",
      "text\n",
      "\n",
      "\"text\"\n",
      "\n",
      "Transformations#\n",
      "\n",
      "After the data is loaded, you then need to process and transform your data before putting it into a storage system. These transformations include chunking, extracting metadata, and embedding each chunk. This is necessary to make sure that the data can be retrieved, and used optimally by the LLM.\n",
      "\n",
      "Transformation input/outputs are Node objects (a Document is a subclass of a Node). Transformations can also be stacked and reordered.\n",
      "\n",
      "We have both a high-level and lower-level API for transforming documents.\n",
      "\n",
      "High-Level Transformation API#\n",
      "\n",
      "Indexes have a .from_documents() method which accepts an array of Document objects and will correctly parse and chunk them up. However, sometimes you will want greater control over how your documents are split up.\n",
      "\n",
      "from\n",
      "\n",
      "llama_index.core\n",
      "\n",
      "import\n",
      "\n",
      "VectorStoreIndex\n",
      "\n",
      "vector_index\n",
      "\n",
      "VectorStoreIndex\n",
      "\n",
      "from_documents\n",
      "\n",
      "documents\n",
      "\n",
      "vector_index\n",
      "\n",
      "as_query_engine\n",
      "\n",
      "()\n",
      "\n",
      "Under the hood, this splits your Document into Node objects, which are similar to Documents (they contain text and metadata) but have a relationship to their parent Document.\n",
      "\n",
      "If you want to customize core components, like the text splitter, through this abstraction you can pass in a custom transformations list or apply to the global Settings:\n",
      "\n",
      "text_splitter\n",
      "\n",
      "SentenceSplitter\n",
      "\n",
      "chunk_size\n",
      "\n",
      "512\n",
      "\n",
      "chunk_overlap\n",
      "\n",
      "10\n",
      "\n",
      "# global\n",
      "\n",
      "from\n",
      "\n",
      "llama_index.core\n",
      "\n",
      "import\n",
      "\n",
      "Settings\n",
      "\n",
      "Settings\n",
      "\n",
      "text_splitter\n",
      "\n",
      "text_splitter\n",
      "\n",
      "# per-index\n",
      "\n",
      "index\n",
      "\n",
      "VectorStoreIndex\n",
      "\n",
      "from_documents\n",
      "\n",
      "documents\n",
      "\n",
      "transformations\n",
      "\n",
      "text_splitter\n",
      "\n",
      "Lower-Level Transformation API#\n",
      "\n",
      "You can also define these steps explicitly.\n",
      "\n",
      "You can do this by either using our transformation modules (text splitters, metadata extractors, etc.) as standalone components, or compose them in our declarative Transformation Pipeline interface.\n",
      "\n",
      "Let's walk through the steps below.\n",
      "\n",
      "Splitting Your Documents into Nodes#\n",
      "\n",
      "A key step to process your documents is to split them into \"chunks\"/Node objects. The key idea is to process your data into bite-sized pieces that can be retrieved / fed to the LLM.\n",
      "\n",
      "LlamaIndex has support for a wide range of text splitters, ranging from paragraph/sentence/token based splitters to file-based splitters like HTML, JSON.\n",
      "\n",
      "These can be used on their own or as part of an ingestion pipeline.\n",
      "\n",
      "from\n",
      "\n",
      "llama_index.core\n",
      "\n",
      "import\n",
      "\n",
      "SimpleDirectoryReader\n",
      "\n",
      "from\n",
      "\n",
      "llama_index.core.ingestion\n",
      "\n",
      "import\n",
      "\n",
      "IngestionPipeline\n",
      "\n",
      "from\n",
      "\n",
      "llama_index.core.node_parser\n",
      "\n",
      "import\n",
      "\n",
      "TokenTextSplitter\n",
      "\n",
      "documents\n",
      "\n",
      "SimpleDirectoryReader\n",
      "\n",
      "\"./data\"\n",
      "\n",
      "load_data\n",
      "\n",
      "()\n",
      "\n",
      "pipeline\n",
      "\n",
      "IngestionPipeline\n",
      "\n",
      "transformations\n",
      "\n",
      "TokenTextSplitter\n",
      "\n",
      "(),\n",
      "\n",
      "...\n",
      "\n",
      "])\n",
      "\n",
      "nodes\n",
      "\n",
      "pipeline\n",
      "\n",
      "run\n",
      "\n",
      "documents\n",
      "\n",
      "documents\n",
      "\n",
      "Adding Metadata#\n",
      "\n",
      "You can also choose to add metadata to your documents and nodes. This can be done either manually or with automatic metadata extractors.\n",
      "\n",
      "Here are guides on 1) how to customize Documents, and 2) how to customize Nodes.\n",
      "\n",
      "document\n",
      "\n",
      "Document\n",
      "\n",
      "text\n",
      "\n",
      "\"text\"\n",
      "\n",
      "metadata\n",
      "\n",
      "\"filename\"\n",
      "\n",
      "\"<doc_file_name>\"\n",
      "\n",
      "\"category\"\n",
      "\n",
      "\"<category>\"\n",
      "\n",
      "},\n",
      "\n",
      "Adding Embeddings#\n",
      "\n",
      "To insert a node into a vector index, it should have an embedding. See our ingestion pipeline or our embeddings guide for more details.\n",
      "\n",
      "Creating and passing Nodes directly#\n",
      "\n",
      "If you want to, you can create nodes directly and pass a list of Nodes directly to an indexer:\n",
      "\n",
      "from\n",
      "\n",
      "llama_index.core.schema\n",
      "\n",
      "import\n",
      "\n",
      "TextNode\n",
      "\n",
      "node1\n",
      "\n",
      "TextNode\n",
      "\n",
      "text\n",
      "\n",
      "\"<text_chunk>\"\n",
      "\n",
      "id_\n",
      "\n",
      "\"<node_id>\"\n",
      "\n",
      "node2\n",
      "\n",
      "TextNode\n",
      "\n",
      "text\n",
      "\n",
      "\"<text_chunk>\"\n",
      "\n",
      "id_\n",
      "\n",
      "\"<node_id>\"\n",
      "\n",
      "index\n",
      "\n",
      "VectorStoreIndex\n",
      "\n",
      "([\n",
      "\n",
      "node1\n",
      "\n",
      "node2\n",
      "\n",
      "])\n",
      "\n",
      "\n",
      "\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/evaluating/evaluating/index.html\n",
      "import\n",
      "\n",
      "RetrieverEvaluator\n",
      "\n",
      "# define retriever somewhere (e.g. from index)\n",
      "\n",
      "# retriever = index.as_retriever(similarity_top_k=2)\n",
      "\n",
      "retriever\n",
      "\n",
      "...\n",
      "\n",
      "retriever_evaluator\n",
      "\n",
      "RetrieverEvaluator\n",
      "\n",
      "from_metric_names\n",
      "\n",
      "\"mrr\"\n",
      "\n",
      "\"hit_rate\"\n",
      "\n",
      "],\n",
      "\n",
      "retriever\n",
      "\n",
      "retriever\n",
      "\n",
      "retriever_evaluator\n",
      "\n",
      "evaluate\n",
      "\n",
      "query\n",
      "\n",
      "\"query\"\n",
      "\n",
      "expected_ids\n",
      "\n",
      "\"node_id1\"\n",
      "\n",
      "\"node_id2\"\n",
      "\n",
      "This compares what was retrieved for the query to a set of nodes that were expected to be retrieved.\n",
      "\n",
      "In reality you would want to evaluate a whole batch of retrievals; you can learn how do this in our module guide on retrieval evaluation.\n",
      "\n",
      "Related concepts#\n",
      "\n",
      "You may be interested in analyzing the cost of your application if you are making calls to a hosted, remote LLM.\n",
      "\n",
      "\n",
      "\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/evaluating/cost_analysis/index.html\n",
      "# use a mock embedding globally\n",
      "\n",
      "Settings\n",
      "\n",
      "embed_model\n",
      "\n",
      "MockEmbedding\n",
      "\n",
      "embed_dim\n",
      "\n",
      "1536\n",
      "\n",
      "Usage Pattern#\n",
      "\n",
      "Read about the full usage pattern for more details!\n",
      "\n",
      "\n",
      "\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/evaluating/cost_analysis/usage_pattern/index.html\n",
      "SimpleDirectoryReader\n",
      "\n",
      "\"./docs/examples/data/paul_graham\"\n",
      "\n",
      "load_data\n",
      "\n",
      "()\n",
      "\n",
      "index\n",
      "\n",
      "VectorStoreIndex\n",
      "\n",
      "from_documents\n",
      "\n",
      "documents\n",
      "\n",
      "Measure the counts!\n",
      "\n",
      "print\n",
      "\n",
      "\"Embedding Tokens: \"\n",
      "\n",
      "token_counter\n",
      "\n",
      "total_embedding_token_count\n",
      "\n",
      "\\n\n",
      "\n",
      "\"LLM Prompt Tokens: \"\n",
      "\n",
      "token_counter\n",
      "\n",
      "prompt_llm_token_count\n",
      "\n",
      "\\n\n",
      "\n",
      "\"LLM Completion Tokens: \"\n",
      "\n",
      "token_counter\n",
      "\n",
      "completion_llm_token_count\n",
      "\n",
      "\\n\n",
      "\n",
      "\"Total LLM Token Count: \"\n",
      "\n",
      "token_counter\n",
      "\n",
      "total_llm_token_count\n",
      "\n",
      "\\n\n",
      "\n",
      "# reset counts\n",
      "\n",
      "token_counter\n",
      "\n",
      "reset_counts\n",
      "\n",
      "()\n",
      "\n",
      "Run a query, measure again\n",
      "\n",
      "query_engine\n",
      "\n",
      "index\n",
      "\n",
      "as_query_engine\n",
      "\n",
      "()\n",
      "\n",
      "response\n",
      "\n",
      "query_engine\n",
      "\n",
      "query\n",
      "\n",
      "\"query\"\n",
      "\n",
      "print\n",
      "\n",
      "\"Embedding Tokens: \"\n",
      "\n",
      "token_counter\n",
      "\n",
      "total_embedding_token_count\n",
      "\n",
      "\\n\n",
      "\n",
      "\"LLM Prompt Tokens: \"\n",
      "\n",
      "token_counter\n",
      "\n",
      "prompt_llm_token_count\n",
      "\n",
      "\\n\n",
      "\n",
      "\"LLM Completion Tokens: \"\n",
      "\n",
      "token_counter\n",
      "\n",
      "completion_llm_token_count\n",
      "\n",
      "\\n\n",
      "\n",
      "\"Total LLM Token Count: \"\n",
      "\n",
      "token_counter\n",
      "\n",
      "total_llm_token_count\n",
      "\n",
      "\\n\n",
      "\n",
      "\n",
      "\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/storing/storing/index.html\n",
      "# load some documents\n",
      "\n",
      "documents\n",
      "\n",
      "SimpleDirectoryReader\n",
      "\n",
      "\"./data\"\n",
      "\n",
      "load_data\n",
      "\n",
      "()\n",
      "\n",
      "# initialize client, setting path to save data\n",
      "\n",
      "db\n",
      "\n",
      "chromadb\n",
      "\n",
      "PersistentClient\n",
      "\n",
      "path\n",
      "\n",
      "\"./chroma_db\"\n",
      "\n",
      "# create collection\n",
      "\n",
      "chroma_collection\n",
      "\n",
      "db\n",
      "\n",
      "get_or_create_collection\n",
      "\n",
      "\"quickstart\"\n",
      "\n",
      "# assign chroma as the vector_store to the context\n",
      "\n",
      "vector_store\n",
      "\n",
      "ChromaVectorStore\n",
      "\n",
      "chroma_collection\n",
      "\n",
      "chroma_collection\n",
      "\n",
      "storage_context\n",
      "\n",
      "StorageContext\n",
      "\n",
      "from_defaults\n",
      "\n",
      "vector_store\n",
      "\n",
      "vector_store\n",
      "\n",
      "# create your index\n",
      "\n",
      "index\n",
      "\n",
      "VectorStoreIndex\n",
      "\n",
      "from_documents\n",
      "\n",
      "documents\n",
      "\n",
      "storage_context\n",
      "\n",
      "storage_context\n",
      "\n",
      "# create a query engine and query\n",
      "\n",
      "query_engine\n",
      "\n",
      "index\n",
      "\n",
      "as_query_engine\n",
      "\n",
      "()\n",
      "\n",
      "response\n",
      "\n",
      "query_engine\n",
      "\n",
      "query\n",
      "\n",
      "\"What is the meaning of life?\"\n",
      "\n",
      "print\n",
      "\n",
      "response\n",
      "\n",
      "If you've already created and stored your embeddings, you'll want to load them directly without loading your documents or creating a new VectorStoreIndex:\n",
      "\n",
      "import\n",
      "\n",
      "chromadb\n",
      "\n",
      "from\n",
      "\n",
      "llama_index.core\n",
      "\n",
      "import\n",
      "\n",
      "VectorStoreIndex\n",
      "\n",
      "from\n",
      "\n",
      "llama_index.vector_stores.chroma\n",
      "\n",
      "import\n",
      "\n",
      "ChromaVectorStore\n",
      "\n",
      "from\n",
      "\n",
      "llama_index.core\n",
      "\n",
      "import\n",
      "\n",
      "StorageContext\n",
      "\n",
      "# initialize client\n",
      "\n",
      "db\n",
      "\n",
      "chromadb\n",
      "\n",
      "PersistentClient\n",
      "\n",
      "path\n",
      "\n",
      "\"./chroma_db\"\n",
      "\n",
      "# get collection\n",
      "\n",
      "chroma_collection\n",
      "\n",
      "db\n",
      "\n",
      "get_or_create_collection\n",
      "\n",
      "\"quickstart\"\n",
      "\n",
      "# assign chroma as the vector_store to the context\n",
      "\n",
      "vector_store\n",
      "\n",
      "ChromaVectorStore\n",
      "\n",
      "chroma_collection\n",
      "\n",
      "chroma_collection\n",
      "\n",
      "storage_context\n",
      "\n",
      "StorageContext\n",
      "\n",
      "from_defaults\n",
      "\n",
      "vector_store\n",
      "\n",
      "vector_store\n",
      "\n",
      "# load your index from stored vectors\n",
      "\n",
      "index\n",
      "\n",
      "VectorStoreIndex\n",
      "\n",
      "from_vector_store\n",
      "\n",
      "vector_store\n",
      "\n",
      "storage_context\n",
      "\n",
      "storage_context\n",
      "\n",
      "# create a query engine\n",
      "\n",
      "query_engine\n",
      "\n",
      "index\n",
      "\n",
      "as_query_engine\n",
      "\n",
      "()\n",
      "\n",
      "response\n",
      "\n",
      "query_engine\n",
      "\n",
      "query\n",
      "\n",
      "\"What is llama2?\"\n",
      "\n",
      "print\n",
      "\n",
      "response\n",
      "\n",
      "Tip\n",
      "\n",
      "We have a more thorough example of using Chroma if you want to go deeper on this store.\n",
      "\n",
      "You're ready to query!#\n",
      "\n",
      "Now you have loaded data, indexed it, and stored that index, you're ready to query your data.\n",
      "\n",
      "Inserting Documents or Nodes#\n",
      "\n",
      "If you've already created an index, you can add new documents to your index using the insert method.\n",
      "\n",
      "from\n",
      "\n",
      "llama_index.core\n",
      "\n",
      "import\n",
      "\n",
      "VectorStoreIndex\n",
      "\n",
      "index\n",
      "\n",
      "VectorStoreIndex\n",
      "\n",
      "([])\n",
      "\n",
      "for\n",
      "\n",
      "doc\n",
      "\n",
      "in\n",
      "\n",
      "documents\n",
      "\n",
      "index\n",
      "\n",
      "insert\n",
      "\n",
      "doc\n",
      "\n",
      "See the document management how-to for more details on managing documents and an example notebook.\n",
      "\n",
      "\n",
      "\n",
      "/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Agents/docs.llamaindex.ai/en/latest/understanding/tracing_and_debugging/tracing_and_debugging/index.html\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.metadata[\"path\"])\n",
    "    print(doc.get_content())\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Global LLM + Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다중 문서 에이전트 구축\n",
    "이 섹션에서는 다중 문서 에이전트를 구성하는 방법을 보여줍니다. \n",
    "\n",
    "먼저 각 문서에 대한 문서 에이전트를 구축한 다음 개체 인덱스를 사용하여 최상위 상위 에이전트를 정의합니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 각 문서에 대한 문서 에이전트 구축\n",
    "이 섹션에서는 각 문서에 대한 \"문서 에이전트\"를 정의합니다.\n",
    "\n",
    "우리는 각 문서에 대해 벡터 인덱스(의미 검색용)와 요약 인덱스(요약용)를 모두 정의합니다. 그런 다음 두 개의 쿼리 엔진은 OpenAI 함수 호출 에이전트에 전달되는 도구로 변환됩니다.\n",
    "\n",
    "이 문서 에이전트는 주어진 문서 내에서 의미 검색 또는 요약을 수행하도록 동적으로 선택할 수 있습니다.\n",
    "\n",
    "우리는 각 도시마다 별도의 문서 에이전트를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core import (\n",
    "    load_index_from_storage,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "async def build_agent_per_doc(nodes, file_base):\n",
    "    print(file_base)\n",
    "\n",
    "    vi_out_path = f\"./data/llamaindex_docs/{file_base}\"\n",
    "    summary_out_path = f\"./data/llamaindex_docs/{file_base}_summary.pkl\"\n",
    "    if not os.path.exists(vi_out_path):\n",
    "        Path(\"./data/llamaindex_docs/\").mkdir(parents=True, exist_ok=True)\n",
    "        # build vector index\n",
    "        vector_index = VectorStoreIndex(nodes)\n",
    "        vector_index.storage_context.persist(persist_dir=vi_out_path)\n",
    "    else:\n",
    "        vector_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=vi_out_path),\n",
    "        )\n",
    "\n",
    "    # build summary index\n",
    "    summary_index = SummaryIndex(nodes)\n",
    "\n",
    "    # define query engines\n",
    "    vector_query_engine = vector_index.as_query_engine(llm=llm)\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\", llm=llm\n",
    "    )\n",
    "\n",
    "    # extract a summary\n",
    "    if not os.path.exists(summary_out_path):\n",
    "        Path(summary_out_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        summary = str(\n",
    "            await summary_query_engine.aquery(\n",
    "                \"Extract a concise 1-2 line summary of this document\"\n",
    "            )\n",
    "        )\n",
    "        pickle.dump(summary, open(summary_out_path, \"wb\"))\n",
    "    else:\n",
    "        summary = pickle.load(open(summary_out_path, \"rb\"))\n",
    "\n",
    "    # define tools\n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=vector_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=f\"vector_tool_{file_base}\",\n",
    "                description=f\"Useful for questions related to specific facts\",\n",
    "            ),\n",
    "        ),\n",
    "        QueryEngineTool(\n",
    "            query_engine=summary_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=f\"summary_tool_{file_base}\",\n",
    "                description=f\"Useful for summarization questions\",\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # build agent\n",
    "    function_llm = OpenAI(model=\"gpt-4\")\n",
    "    agent = OpenAIAgent.from_tools(\n",
    "        query_engine_tools,\n",
    "        llm=function_llm,\n",
    "        verbose=True,\n",
    "        system_prompt=f\"\"\"\\\n",
    "You are a specialized agent designed to answer queries about the `{file_base}.html` part of the LlamaIndex docs.\n",
    "You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    return agent, summary\n",
    "\n",
    "\n",
    "async def build_agents(docs):\n",
    "    node_parser = SentenceSplitter()\n",
    "\n",
    "    # Build agents dictionary\n",
    "    agents_dict = {}\n",
    "    extra_info_dict = {}\n",
    "\n",
    "    # # this is for the baseline\n",
    "    # all_nodes = []\n",
    "\n",
    "    for idx, doc in enumerate(tqdm(docs)):\n",
    "    # for idx, doc in enumerate(docs):\n",
    "        nodes = node_parser.get_nodes_from_documents([doc])\n",
    "        # all_nodes.extend(nodes)\n",
    "\n",
    "        # ID will be base + parent\n",
    "        file_path = Path(doc.metadata[\"path\"])\n",
    "        file_base = str(file_path.parent.stem) + \"_\" + str(file_path.stem)\n",
    "        agent, summary = await build_agent_per_doc(nodes, file_base)\n",
    "\n",
    "        agents_dict[file_base] = agent\n",
    "        extra_info_dict[file_base] = {\"summary\": summary, \"nodes\": nodes}\n",
    "\n",
    "    return agents_dict, extra_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16c2d9fab4d4d3c8fcd0565846e718f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest_index\n",
      "understanding_index\n",
      "using_llms_index\n",
      "privacy_index\n",
      "llamahub_index\n",
      "loading_index\n",
      "evaluating_index\n",
      "cost_analysis_index\n",
      "usage_pattern_index\n",
      "storing_index\n",
      "tracing_and_debugging_index\n",
      "indexing_index\n",
      "querying_index\n",
      "putting_it_all_together_index\n",
      "q_and_a_index\n",
      "terms_definitions_tutorial_index\n",
      "agents_index\n",
      "structured_data_index\n",
      "Airbyte_demo_index\n",
      "building_a_chatbot_index\n",
      "graphs_index\n",
      "apps_index\n",
      "fullstack_with_delphic_index\n",
      "fullstack_app_guide_index\n",
      "api_reference_index\n",
      "retrievers_index\n",
      "tree_index\n",
      "auto_merging_index\n",
      "knowledge_graph_index\n",
      "videodb_index\n",
      "query_fusion_index\n",
      "recursive_index\n",
      "bm25_index\n",
      "pathway_index\n",
      "you_index\n",
      "keyword_index\n",
      "vector_index\n",
      "transform_index\n",
      "summary_index\n",
      "router_index\n",
      "sql_index\n",
      "ingestion_index\n",
      "indices_index\n",
      "tree_index\n",
      "knowledge_graph_index\n",
      "llama_cloud_index\n",
      "google_index\n",
      "vectara_index\n",
      "colbert_index\n",
      "document_summary_index\n",
      "zilliz_index\n",
      "keyword_index\n",
      "vector_index\n",
      "summary_index\n",
      "tools_index\n",
      "multion_index\n",
      "waii_index\n",
      "neo4j_index\n",
      "database_index\n",
      "metaphor_index\n",
      "wikipedia_index\n",
      "passio_nutrition_ai_index\n",
      "python_file_index\n",
      "azure_translate_index\n",
      "duckduckgo_index\n",
      "vector_db_index\n",
      "azure_cv_index\n",
      "zapier_index\n",
      "google_index\n",
      "exa_index\n",
      "yelp_index\n",
      "shopify_index\n",
      "function_index\n",
      "openapi_index\n",
      "retriever_index\n",
      "tavily_research_index\n",
      "notion_index\n",
      "requests_index\n",
      "wolfram_alpha_index\n",
      "ionic_shopping_index\n",
      "yahoo_finance_index\n",
      "bing_search_index\n",
      "graphql_index\n",
      "playgrounds_index\n",
      "text_to_image_index\n",
      "brave_search_index\n",
      "azure_speech_index\n",
      "salesforce_index\n",
      "finance_index\n",
      "slack_index\n",
      "weather_index\n",
      "arxiv_index\n",
      "load_and_search_index\n",
      "ondemand_loader_index\n",
      "query_engine_index\n",
      "chatgpt_plugin_index\n",
      "cogniswitch_index\n",
      "tool_spec_index\n",
      "code_interpreter_index\n",
      "openai_index\n",
      "query_plan_index\n",
      "embeddings_index\n",
      "gradient_index\n",
      "voyageai_index\n",
      "cohere_index\n",
      "jinaai_index\n",
      "together_index\n",
      "mistralai_index\n",
      "huggingface_optimum_index\n",
      "gemini_index\n",
      "azure_openai_index\n",
      "anyscale_index\n",
      "langchain_index\n",
      "clip_index\n",
      "google_index\n",
      "fireworks_index\n",
      "fastembed_index\n",
      "adapter_index\n",
      "nomic_index\n",
      "bedrock_index\n",
      "llamafile_index\n",
      "text_embeddings_inference_index\n",
      "sagemaker_endpoint_index\n",
      "llm_rails_index\n",
      "dashscope_index\n",
      "huggingface_index\n",
      "vertex_index\n",
      "clarifai_index\n",
      "ollama_index\n",
      "alephalpha_index\n",
      "huggingface_optimum_intel_index\n",
      "instructor_index\n",
      "premai_index\n",
      "elasticsearch_index\n",
      "openai_index\n",
      "memory_index\n",
      "chat_memory_buffer_index\n",
      "callbacks_index\n",
      "langfuse_index\n",
      "llama_debug_index\n",
      "token_counter_index\n",
      "openinference_index\n",
      "promptlayer_index\n",
      "honeyhive_index\n",
      "argilla_index\n",
      "deepeval_index\n",
      "arize_phoenix_index\n",
      "uptrain_index\n",
      "aim_index\n",
      "wandb_index\n",
      "chat_engines_index\n",
      "condense_plus_context_index\n",
      "context_index\n",
      "simple_index\n",
      "condense_question_index\n",
      "postprocessor_index\n",
      "llm_rerank_index\n",
      "sentence_optimizer_index\n",
      "PII_index\n",
      "NER_PII_index\n",
      "metadata_replacement_index\n",
      "similarity_index\n",
      "jinaai_rerank_index\n",
      "fixed_recency_index\n",
      "auto_prev_next_index\n",
      "embedding_recency_index\n",
      "longllmlingua_index\n",
      "prev_next_index\n",
      "presidio_index\n",
      "long_context_reorder_index\n",
      "sbert_rerank_index\n",
      "colbert_rerank_index\n",
      "time_weighted_index\n",
      "rankgpt_rerank_index\n",
      "keyword_index\n",
      "flag_embedding_reranker_index\n",
      "cohere_rerank_index\n",
      "multi_modal_llms_index\n",
      "gemini_index\n",
      "azure_openai_index\n",
      "replicate_index\n",
      "anthropic_index\n",
      "dashscope_index\n",
      "ollama_index\n",
      "openai_index\n",
      "objects_index\n",
      "agent_index\n",
      "openai_legacy_index\n",
      "react_index\n",
      "openai_index\n",
      "query_pipeline_index\n",
      "tool_runner_index\n",
      "output_parser_index\n",
      "llm_index\n",
      "input_index\n",
      "postprocessor_index\n",
      "query_transform_index\n",
      "arg_pack_index\n",
      "agent_index\n",
      "function_index\n",
      "retriever_index\n"
     ]
    }
   ],
   "source": [
    "agents_dict, extra_info_dict = await build_agents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리트리버 지원 OpenAI 에이전트 빌드\n",
    "우리는 다양한 문서 에이전트를 조정하여 모든 사용자 쿼리에 응답할 수 있는 최상위 에이전트를 구축합니다.\n",
    "\n",
    "이는 RetrieverOpenAIAgent도구를 사용하기 전에 도구 검색을 수행합니다(모든 도구를 프롬프트에 넣으려고 시도하는 기본 에이전트와는 다름).\n",
    "\n",
    "V0의 개선 사항 : V0의 \"기본\" 버전과 비교하여 다음과 같은 개선 사항을 적용했습니다.\n",
    "- 순위 재지정 기능 추가: Cohere reranker를 사용하여 후보 문서 세트를 더 효과적으로 필터링합니다.\n",
    "- 쿼리 계획 도구 추가: 검색된 도구 집합을 기반으로 동적으로 생성되는 명시적 쿼리 계획 도구를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tool for each document agent\n",
    "all_tools = []\n",
    "for file_base, agent in agents_dict.items():\n",
    "    summary = extra_info_dict[file_base][\"summary\"]\n",
    "    doc_tool = QueryEngineTool(\n",
    "        query_engine=agent,\n",
    "        metadata=ToolMetadata(\n",
    "            name=f\"tool_{file_base}\",\n",
    "            description=summary,\n",
    "        ),\n",
    "    )\n",
    "    all_tools.append(doc_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolMetadata(description='This document provides guidance on tracing and debugging processes for effective troubleshooting in software development.', name='tool_tracing_and_debugging_index', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>)\n",
      "182\n"
     ]
    }
   ],
   "source": [
    "print(all_tools[10].metadata)\n",
    "print(len(all_tools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import (\n",
    "    ObjectIndex,\n",
    "    SimpleToolNodeMapping,\n",
    "    ObjectRetriever,\n",
    ")\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4-0613\")\n",
    "\n",
    "tool_mapping = SimpleToolNodeMapping.from_objects(all_tools)\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    tool_mapping,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "vector_node_retriever = obj_index.as_node_retriever(similarity_top_k=10)\n",
    "\n",
    "\n",
    "# define a custom retriever with reranking\n",
    "class CustomRetriever(BaseRetriever):\n",
    "    def __init__(self, vector_retriever, postprocessor=None):\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._postprocessor = postprocessor or CohereRerank(top_n=5)\n",
    "        # self._postprocessor = postprocessor\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle):\n",
    "        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        filtered_nodes = self._postprocessor.postprocess_nodes(\n",
    "            retrieved_nodes, query_bundle=query_bundle\n",
    "        )\n",
    "\n",
    "        return filtered_nodes\n",
    "\n",
    "\n",
    "# define a custom object retriever that adds in a query planning tool\n",
    "class CustomObjectRetriever(ObjectRetriever):\n",
    "    def __init__(self, retriever, object_node_mapping, all_tools, llm=None):\n",
    "        self._retriever = retriever\n",
    "        self._object_node_mapping = object_node_mapping\n",
    "        self._llm = llm or OpenAI(\"gpt-4-0613\")\n",
    "\n",
    "    def retrieve(self, query_bundle):\n",
    "        nodes = self._retriever.retrieve(query_bundle)\n",
    "        tools = [self._object_node_mapping.from_node(n.node) for n in nodes]\n",
    "\n",
    "        sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
    "            query_engine_tools=tools, llm=self._llm\n",
    "        )\n",
    "        sub_question_description = f\"\"\"\\\n",
    "Useful for any queries that involve comparing multiple documents. ALWAYS use this tool for comparison queries - make sure to call this \\\n",
    "tool with the original query. Do NOT use the other tools for any queries involving multiple documents.\n",
    "\"\"\"\n",
    "        sub_question_tool = QueryEngineTool(\n",
    "            query_engine=sub_question_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"compare_tool\", description=sub_question_description\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return tools + [sub_question_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# CohereRerank를 대신해서...\n",
    "# postprocessor = SimilarityPostprocessor(similarity_cutoff=0.4)\n",
    "\n",
    "custom_node_retriever = CustomRetriever(vector_node_retriever)\n",
    "\n",
    "# wrap it with ObjectRetriever to return objects\n",
    "custom_obj_retriever = CustomObjectRetriever(\n",
    "    custom_node_retriever, tool_mapping, all_tools, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4-0613'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "tmps = custom_obj_retriever.retrieve(\"how can use your data to generate questions to evaluate on?\")\n",
    "print(len(tmps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai_legacy import FnRetrieverOpenAIAgent\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "top_agent = FnRetrieverOpenAIAgent.from_retriever(\n",
    "    custom_obj_retriever,\n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries about the documentation.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# top_agent = ReActAgent.from_tools(\n",
    "#     tool_retriever=custom_obj_retriever,\n",
    "#     system_prompt=\"\"\" \\\n",
    "# You are an agent designed to answer queries about the documentation.\n",
    "# Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "# \"\"\",\n",
    "#     llm=llm,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 도구함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_source_nodes(response):\n",
    "    for node in response.source_nodes:\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_llm_result(question, base_txt, agent_txt):\n",
    "    # 파일을 추가 모드('a')로 열기\n",
    "    with open('평가:Llama-index Document.md', 'a') as file:\n",
    "        # 파일에 텍스트 추가\n",
    "        file.write((\n",
    "            f\"\\n\\n------------------------------------------------------------------------------------------------------------------------------------------\"\n",
    "            f\"\\n\\n### Q. {question}\"\n",
    "            f\"\\n##### Base:\"\n",
    "            f\"\\n{base_txt}\"\n",
    "            f\"\\n\\n##### Agent:\"\n",
    "            f\"\\n{agent_txt}\"\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 벡터 저장소 인덱스 정의\n",
    "비교를 위해 모든 문서를 단일 벡터 인덱스 컬렉션으로 덤프하는 \"순진한\" RAG 파이프라인을 정의합니다.\n",
    "\n",
    "top_k = 4로 설정했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = [\n",
    "    n for extra_info in extra_info_dict.values() for n in extra_info[\"nodes\"]\n",
    "]\n",
    "\n",
    "base_index = VectorStoreIndex(all_nodes)\n",
    "base_query_engine = base_index.as_query_engine(similarity_top_k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 쿼리 실행\n",
    "\n",
    "단일 문서에 대한 QA/요약부터 여러 문서에 대한 QA/요약에 이르기까지 몇 가지 예시 쿼리를 실행해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    # \"Multi-Document Agents의 용도는 무엇이고 어떠한 usecase가 있는지 알려줘\",\n",
    "    # \"Multi-Document Agents 사용에서 CohereRerank가 하는 역할이 무엇이고 이것이 미치는 영향이 큰가?\",\n",
    "    # \"Llama-index에서 \\\"Response Synthesizer\\\"가 무엇이고 이것이 하는 역할과 Usecase를 알려줘\",\n",
    "    # \"IndexNode가 무엇이고 이것이 하는 역할과 Usecase를 알려줘\",\n",
    "    # \"ColbertRerank vs CohereRerank 두개를 비교했을때 장단점을 비교해주고 특히 RAG의 성능에 미치는 영향에 대해서 알려줘\",\n",
    "    # \"FnRetrieverOpenAIAgent와 ReActAgent에 대해서 알려주고 이 두개의 특징을 비교해서 알려줘\",\n",
    "    \"Llama-index에서 SentenceTransformerRerank에 대해서 설명해주고 CohereRerank와 어떤 차이점이 있는지 알려줘\",\n",
    "    \"SentenceTransformerRerank의 Usecase에 대해서 알려줘\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: compare_tool with args: {\n",
      "  \"input\": \"Llama-index에서 SentenceTransformerRerank에 대해서 설명해주고 CohereRerank와 어떤 차이점이 있는지 알려줘\"\n",
      "}\n",
      "Generated 6 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[tool_sentence_optimizer_index] Q: Llama-index에서 SentenceTransformerRerank에 대한 최적화 정보는 무엇인가요?\n",
      "\u001b[0mAdded user message to memory: Llama-index에서 SentenceTransformerRerank에 대한 최적화 정보는 무엇인가요?\n",
      "\u001b[1;3;38;2;90;149;237m[tool_llm_rerank_index] Q: Llama-index에서 SentenceTransformerRerank의 후처리 단계는 어떻게 진행되나요?\n",
      "\u001b[0mAdded user message to memory: Llama-index에서 SentenceTransformerRerank의 후처리 단계는 어떻게 진행되나요?\n",
      "\u001b[1;3;38;2;11;159;203m[tool_transform_index] Q: Llama-index에서 SentenceTransformerRerank를 변환하는 방법은 무엇인가요?\n",
      "\u001b[0mAdded user message to memory: Llama-index에서 SentenceTransformerRerank를 변환하는 방법은 무엇인가요?\n",
      "\u001b[1;3;38;2;155;135;227m[tool_cohere_index] Q: Llama-index에서 CohereRerank의 임베딩 일관성 정보는 무엇인가요?\n",
      "\u001b[0mAdded user message to memory: Llama-index에서 CohereRerank의 임베딩 일관성 정보는 무엇인가요?\n",
      "\u001b[1;3;38;2;237;90;200m[tool_rankgpt_rerank_index] Q: Llama-index에서 CohereRerank의 후처리 단계는 어떻게 진행되나요?\n",
      "\u001b[0mAdded user message to memory: Llama-index에서 CohereRerank의 후처리 단계는 어떻게 진행되나요?\n",
      "\u001b[1;3;38;2;90;149;237m[tool_cohere_index] Q: Llama-index에서 SentenceTransformerRerank와 CohereRerank의 차이점은 무엇인가요?\n",
      "\u001b[0mAdded user message to memory: Llama-index에서 SentenceTransformerRerank와 CohereRerank의 차이점은 무엇인가요?\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_cohere_index with args: {\n",
      "  \"input\": \"CohereRerank embedding consistency information\"\n",
      "}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_transform_index with args: {\n",
      "  \"input\": \"SentenceTransformerRerank\"\n",
      "}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_rankgpt_rerank_index with args: {\n",
      "  \"input\": \"CohereRerank post-processing\"\n",
      "}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_cohere_index with args: {\n",
      "  \"input\": \"SentenceTransformerRerank CohereRerank\"\n",
      "}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_sentence_optimizer_index with args: {\n",
      "  \"input\": \"SentenceTransformerRerank\"\n",
      "}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_llm_rerank_index with args: {\n",
      "  \"input\": \"SentenceTransformerRerank post-processing\"\n",
      "}\n",
      "Got output: SentenceTransformerRerank\n",
      "========================\n",
      "\n",
      "Got output: SentenceTransformerRerank is a term related to the API reference for retrievers in the documentation.\n",
      "========================\n",
      "\n",
      "Got output: To find information on CohereRerank embedding consistency, you would need to refer to the documentation located at the specified path.\n",
      "========================\n",
      "\n",
      "Got output: CohereRerank post-processing is a step in the post-processing pipeline that involves utilizing the CohereRerank algorithm to refine and improve the ranking of search results generated by the initial processing steps.\n",
      "========================\n",
      "\n",
      "Got output: The post-processing for SentenceTransformerRerank involves additional steps to refine the results generated by the SentenceTransformer model.\n",
      "========================\n",
      "\n",
      "Got output: The SentenceTransformerRerank and CohereRerank are components related to embeddings in the API reference documentation for the llama-index project.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_transform_index with args: {\n",
      "  \"input\": \"SentenceTransformerRerank\"\n",
      "}\n",
      "\u001b[1;3;38;2;155;135;227m[tool_cohere_index] A: 죄송합니다, 현재 제가 제공할 수 있는 정보가 없습니다. CohereRerank의 임베딩 일관성 정보에 대한 자세한 내용은 LlamaIndex 문서의 `cohere_index.html` 부분을 참조해 주세요.\n",
      "\u001b[0mGot output: SentenceTransformerRerank is a feature or functionality related to the SentenceTransformer model that is used for reranking sentences or text based on certain criteria or similarity measures.\n",
      "========================\n",
      "\n",
      "\u001b[1;3;38;2;237;90;200m[tool_rankgpt_rerank_index] A: Llama-index에서 CohereRerank의 후처리 단계는 초기 처리 단계에서 생성된 검색 결과의 순위를 세밀하게 조정하고 개선하기 위해 CohereRerank 알고리즘을 사용하는 과정입니다. 이 단계는 후처리 파이프라인의 일부입니다.\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[tool_llm_rerank_index] A: SentenceTransformerRerank의 후처리 단계는 SentenceTransformer 모델에 의해 생성된 결과를 세밀하게 조정하는 추가 단계를 포함합니다. 이 단계는 결과의 정확성과 관련성을 높이는 데 도움이 됩니다.\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[tool_sentence_optimizer_index] A: `SentenceTransformerRerank`는 LlamaIndex의 일부로, 문장 임베딩을 사용하여 문장 간의 유사성을 계산하는 데 사용됩니다. 이는 문장 또는 문서의 유사성을 평가하거나, 특정 문장이나 문서에 가장 가까운 다른 문장이나 문서를 찾는 데 유용합니다. 이 기능은 문장 임베딩 모델을 사용하여 문장을 벡터로 변환하고, 이 벡터 간의 유사성을 계산하여 결과를 반환합니다. 이러한 유사성 점수는 문장이나 문서 간의 유사성을 나타내며, 높은 점수는 높은 유사성을 의미합니다.\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[tool_transform_index] A: `SentenceTransformerRerank`는 SentenceTransformer 모델과 관련된 기능이며, 특정 기준이나 유사성 측정에 따라 문장이나 텍스트를 재정렬하는 데 사용됩니다. 이 기능은 LlamaIndex의 `transform_index.html` 부분에서 설명되어 있습니다. 이를 변환하려면 해당 기능에 대한 적절한 파라미터를 설정하고, SentenceTransformer 모델을 사용하여 문장이나 텍스트를 재정렬하는 방법을 적용해야 합니다.\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[tool_cohere_index] A: `SentenceTransformerRerank`와 `CohereRerank`는 Llama-index 프로젝트의 API 참조 문서에서 임베딩과 관련된 구성 요소입니다. \n",
      "\n",
      "`SentenceTransformerRerank`는 문장 변환 모델을 사용하여 문장 임베딩을 생성하고, 이를 기반으로 문서를 재정렬합니다. 이는 문장의 의미를 캡처하고 유사한 문장을 찾는 데 유용합니다.\n",
      "\n",
      "반면에 `CohereRerank`는 Cohere 모델을 사용하여 문장 임베딩을 생성하고, 이를 기반으로 문서를 재정렬합니다. Cohere는 문장의 의미를 더 깊이 이해하고, 더 복잡한 문맥을 처리할 수 있습니다.\n",
      "\n",
      "따라서, 두 방법의 주요 차이점은 사용하는 임베딩 모델입니다. `SentenceTransformerRerank`는 문장 변환 모델을, `CohereRerank`는 Cohere 모델을 사용합니다.\n",
      "\u001b[0mGot output: SentenceTransformerRerank는 LlamaIndex의 일부로서, 문장 임베딩을 사용하여 문장 간의 유사성을 계산하는 기능입니다. 이는 문장 또는 문서의 유사성을 평가하거나, 특정 문장이나 문서에 가장 가까운 다른 문장이나 문서를 찾는 데 유용합니다. 이 기능은 문장 임베딩 모델을 사용하여 문장을 벡터로 변환하고, 이 벡터 간의 유사성을 계산하여 결과를 반환합니다. 높은 유사성 점수는 높은 유사성을 의미합니다.\n",
      "\n",
      "SentenceTransformerRerank와 CohereRerank의 주요 차이점은 사용하는 임베딩 모델입니다. SentenceTransformerRerank는 문장 변환 모델을 사용하여 문장 임베딩을 생성하고, 이를 기반으로 문서를 재정렬합니다. 반면에 CohereRerank는 Cohere 모델을 사용하여 문장 임베딩을 생성하고, 이를 기반으로 문서를 재정렬합니다. Cohere는 문장의 의미를 더 깊이 이해하고, 더 복잡한 문맥을 처리할 수 있습니다.\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n",
      "Node ID: bdb076c4-7d36-461b-882c-7bd8c4457f20\n",
      "Text: Sub question: Llama-index에서 SentenceTransformerRerank에 대한 최적화\n",
      "정보는 무엇인가요? Response: `SentenceTransformerRerank`는 LlamaIndex의 일부로, 문장\n",
      "임베딩을 사용하여 문장 간의 유사성을 계산하는 데 사용됩니다. 이는 문장 또는 문서의 유사성을 평가하거나, 특정 문장이나\n",
      "문서에 가장 가까운 다른 문장이나 문서를 찾는 데 유용합니다. 이 기능은 문장 임베딩 모델을 사용하여 문장을 벡터로 변환하고,\n",
      "이 벡터 간의 유사성을 계산하여 결과를 반환합니다. 이러한 유사성 점수는 문장이나 문서 간의 유사성을 나타내며, 높은 점수는\n",
      "높은 유사성...\n",
      "Score: None\n",
      "\n",
      "Node ID: fba1efe1-9984-424a-8435-476a3be18dfa\n",
      "Text: Sub question: Llama-index에서 SentenceTransformerRerank의 후처리 단계는\n",
      "어떻게 진행되나요? Response: SentenceTransformerRerank의 후처리 단계는\n",
      "SentenceTransformer 모델에 의해 생성된 결과를 세밀하게 조정하는 추가 단계를 포함합니다. 이 단계는 결과의\n",
      "정확성과 관련성을 높이는 데 도움이 됩니다.\n",
      "Score: None\n",
      "\n",
      "Node ID: 69813c0f-315e-40f5-8d87-3d50dce02cd8\n",
      "Text: Sub question: Llama-index에서 SentenceTransformerRerank를 변환하는 방법은\n",
      "무엇인가요? Response: `SentenceTransformerRerank`는 SentenceTransformer 모델과\n",
      "관련된 기능이며, 특정 기준이나 유사성 측정에 따라 문장이나 텍스트를 재정렬하는 데 사용됩니다. 이 기능은\n",
      "LlamaIndex의 `transform_index.html` 부분에서 설명되어 있습니다. 이를 변환하려면 해당 기능에 대한\n",
      "적절한 파라미터를 설정하고, SentenceTransformer 모델을 사용하여 문장이나 텍스트를 재정렬하는 방법을 적용해야\n",
      "합니다.\n",
      "Score: None\n",
      "\n",
      "Node ID: 8436f058-333d-49b4-9391-f64db0aa56dc\n",
      "Text: Sub question: Llama-index에서 CohereRerank의 임베딩 일관성 정보는 무엇인가요?\n",
      "Response: 죄송합니다, 현재 제가 제공할 수 있는 정보가 없습니다. CohereRerank의 임베딩 일관성 정보에 대한\n",
      "자세한 내용은 LlamaIndex 문서의 `cohere_index.html` 부분을 참조해 주세요.\n",
      "Score: None\n",
      "\n",
      "Node ID: 625326e1-14e7-4925-b673-2cf1cf77f2d5\n",
      "Text: Sub question: Llama-index에서 CohereRerank의 후처리 단계는 어떻게 진행되나요?\n",
      "Response: Llama-index에서 CohereRerank의 후처리 단계는 초기 처리 단계에서 생성된 검색 결과의\n",
      "순위를 세밀하게 조정하고 개선하기 위해 CohereRerank 알고리즘을 사용하는 과정입니다. 이 단계는 후처리 파이프라인의\n",
      "일부입니다.\n",
      "Score: None\n",
      "\n",
      "Node ID: 38e418e6-d9ca-407c-984f-8c4d4a9a6455\n",
      "Text: Sub question: Llama-index에서 SentenceTransformerRerank와\n",
      "CohereRerank의 차이점은 무엇인가요? Response: `SentenceTransformerRerank`와\n",
      "`CohereRerank`는 Llama-index 프로젝트의 API 참조 문서에서 임베딩과 관련된 구성 요소입니다.\n",
      "`SentenceTransformerRerank`는 문장 변환 모델을 사용하여 문장 임베딩을 생성하고, 이를 기반으로 문서를\n",
      "재정렬합니다. 이는 문장의 의미를 캡처하고 유사한 문장을 찾는 데 유용합니다.  반면에 `CohereRerank`는\n",
      "Cohere 모델을 사용하여 문장 임베딩을 생...\n",
      "Score: None\n",
      "\n",
      "Node ID: 07025aa0-96e8-4695-9ae3-107d3429b68b\n",
      "Text:\n",
      "Score:  0.432\n",
      "\n",
      "Node ID: d8a3b9af-6cbf-4503-8821-427d76e64b2b\n",
      "Text:\n",
      "Score:  0.547\n",
      "\n",
      "Node ID: e583f8f1-a6ec-41b6-8048-2f0031c179ba\n",
      "Text:\n",
      "Score:  0.423\n",
      "\n",
      "Node ID: 61586721-1de7-4baa-88ad-57a2b399878a\n",
      "Text:\n",
      "Score: None\n",
      "\n",
      "Node ID: e1440d42-fbda-488e-b9be-0938e6d37ea8\n",
      "Text:\n",
      "Score:  0.447\n",
      "\n",
      "Node ID: d210bf8b-9bb8-4be2-9284-8038b547bc57\n",
      "Text:\n",
      "Score:  0.536\n",
      "\n",
      "Node ID: e1440d42-fbda-488e-b9be-0938e6d37ea8\n",
      "Text:\n",
      "Score:  0.370\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: tool_sbert_rerank_index with args: {\n",
      "  \"input\": \"Usecase\"\n",
      "}\n",
      "Added user message to memory: Usecase\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_sbert_rerank_index with args: {\n",
      "  \"input\": \"usecase\"\n",
      "}\n",
      "Got output: The use case for a specific tool, technology, or process is the particular situation or scenario in which it is intended to be used or applied. It defines the purpose or objective for which the tool or technology is designed to address.\n",
      "========================\n",
      "\n",
      "Got output: The `sbert_rerank_index.html` is designed to be used in situations where there is a need to rerank search results using Sentence-BERT embeddings. It is particularly useful when you want to improve the relevance of search results by considering the semantic similarity of the search query and the documents in the index.\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n",
      "Node ID: dbcb8db7-0019-4940-8f5b-88ce0f1dcecd\n",
      "Text:\n",
      "Score: None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    base_response = base_query_engine.query(question)\n",
    "    agent_response = top_agent.query(question)\n",
    "    save_llm_result(question, str(base_response), str(agent_response))\n",
    "    pretty_source_nodes(agent_response)\n",
    "    print(f\"\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visa_chatbot1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
