{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://only-wanna.tistory.com/entry/Cosine-Similarity코사인-유사도와-Euclidean-Distance유클리드-거리-알아보기',\n",
       "  'content': 'Cosine 유사도 - 유클리드 거리 사이 관계. 코사인 유사도와 유클리드 거리는 연관이 거의 없어 보입니다. 하지만 수식적으로 정리하면, 둘 사이 관계를 다음과 같이 표현할 수 있습니다. 유클리드 거리의 제곱을 정리하면 다음과 같게 되고. 위의 식은 다음의 식과 ...'},\n",
       " {'url': 'https://docs.likejazz.com/cosine-sim/',\n",
       "  'content': '코사인 유사도의 의미. 01 Jun 2018. 일반적으로 문서간의 유사도를 비교할 때는 코사인 유사도 cosine similarity 를 주로 사용한다. 여기서는 그 동안 습관적으로 사용해오던 코사인 유사도의 의미를 수식과 함께 예제를 통해 살펴보도록 한다. 2018년 6월 1일 초안 작성 ...'},\n",
       " {'url': 'https://techscene.tistory.com/entry/머신러닝-유사도-및-거리-총정리-코사인-유클리디안-자카드-멘하탄-알고리즘',\n",
       "  'content': '안녕하세요, 테크씬입니다. 데이터 분석과 머신러닝 분야에서 유사도와 거리 측정은 매우 중요한 역할을 합니다. 이러한 측정 방법은 다양한 데이터 유형과 문제 상황에 따라 그 중요성이 달라질 수 있으며, 다양한 유사도 및 거리 측정 방법은 각각 고유의 특성과 적용 분야를 가지고 있습니다. 이 ...'},\n",
       " {'url': 'https://codong.tistory.com/35',\n",
       "  'content': '2.1 유클리드 거리(Euclidean distance) 유클리드 거리는 문서 유사도를 구할 때 자카드 유사도나 코사인 유사도만큼 유용한 방법은 아니다. 하지만 여러 가지 방법을 이해하고, 시도해보는 것 자체만으로 다른 개념들을 이해할 때 도움이 되므로 의미가 있다.'},\n",
       " {'url': 'https://222ys.tistory.com/17',\n",
       "  'content': '유클리드 거리는 길이의 영향 받아 A와 C의 유사도가 더 높게 나와버리지만, 코사인 유사도는 내용 (벡터방향)에 영향 받으므로 A와 B의 유사도가 더 높게 나옴. 2. 코사인 유사도 구하기. -1단계: 문서를 벡터화 (DTM, TF-IDF 등) -2단계: 넘파이로 코사인 유사도 함수 ...'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"백터기반의 검색을 할때 유클리드 와 코사인의 차이점은?\"\n",
    "\n",
    "tool.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import (\n",
    "    ContextualCompressionRetriever,\n",
    "    TavilySearchAPIRetriever,\n",
    ")\n",
    "\n",
    "base_tavily_retriever = TavilySearchAPIRetriever(\n",
    "\tk=6, include_raw_content=True, include_images=True\n",
    ")\n",
    "\n",
    "# tavily_retriever = ContextualCompressionRetriever(\n",
    "# \tbase_compressor=pipeline_compressor, base_retriever=base_tavily_retriever\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = base_tavily_retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '[딥러닝 Nlp] 05. 벡터의 유사도(코사인, 유클리드, 자카드)', 'source': 'https://222ys.tistory.com/17', 'score': 0.96354, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "코딩빌런 솜지\n",
      "POWERED BY TISTORY\n",
      "[딥러닝 NLP] 05. 벡터의 유사도(코사인, 유클리드, 자카드)\n",
      "문서의 유사도 : 문서들 간에 동일하거나 비슷한 단어가 공통적으로 사용된 정도\n",
      "성능: 단어 표현 방법(DTM, Word2Vec 등) 과 유사도 기법(유클리드 거리, 코사인 유사도 등) 에 따라 달라짐\n",
      "# 05-01 코사인 유사도(Cosine Similarity)\n",
      "1. 코사인 유사도란?\n",
      "-두 벡터가 가리키는 방향이 유사한 정도를 코사인 각도로  구하는 기법\n",
      "-동일 = 1, 90° = 0, 180° = -1 이므로, -1 이상 1 이하이며  1에 가까울수록 유사\n",
      "-특징: 문서의 길이가 다른 상황에서 비교적 공정한 비교 가능\n",
      "Ex) 문서A, 문서B는 내용 비슷한데 길이는 2배 차이 / 문서A, 문서C는 다른 내용인데 길이는 비슷한 경우\n",
      "유클리드 거리는 길이의 영향 받아 A와 C의 유사도가 더 높게 나와버리지만,\n",
      "코사인 유사도는 내용(벡터방향)에 영향 받으므로 A와 B의 유사도가 더 높게 나옴\n",
      "2. 코사인 유사도 구하기\n",
      "-1단계: 문서를 벡터화 (DTM, TF-IDF 등)\n",
      "-2단계: 넘파이로 코사인 유사도 함수 구현해 계산\n",
      "3. TF-IDF와 코사인유사도로 영화추천시스템 구현\n",
      "-캐글 영화 데이터셋 링크: https://www.kaggle.com/rounakbanik/the-movies-dataset\n",
      "-1단계: TF-IDF 연산시 결측값 존재하면 에러 발생하므로 결측값 처리\n",
      "-2단계: overview 컬럼에 대한 TF-IDF 구하기\n",
      "tfidf = TfidfVectorizer(stop_words='english') tfidf_matrix = tfidf.fit_transform(data['overview'])\n",
      "-3단계: 코사인 유사도 구하기\n",
      "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
      "-4단계: title을 key, 인덱스를 value로 하는 딕셔너리 생성후 overview의 코사인 유사도가 높은 순서대로 정렬하는 함수 생성\n",
      "# 05-02 여러가지 유사도 기법\n",
      "1. 유클리드 거리(Euclidean distance)\n",
      "-2차원에서의 피타고라스 정리를 다차원(총 단어개수)으로 확장하여 계산. 값 작을수록 가깝다(=문서가 유사하다)\n",
      "Ex. 문서 1,2,3의 DTM을 구했더니 단어 4개 ->  비교기준인 문서Q도 4차원 공간에 배치해 각 문서에 대한 거리 구함\n",
      "2. 자카드 유사도(Jaccard similarity)\n",
      "-합집합 중의 교집합의 비율. 0~1 사이 값 가짐(동일하면 1, 공통원소 없다면 0)\n",
      "Ex)\n",
      "문서1 : ['apple', 'banana', 'everyone', 'like', 'likey', 'watch', 'card', 'holder'] 문서2 : ['apple', 'banana', 'coupon', 'passport', 'love', 'you']\n",
      "합집합 단어 개수: 12, 교집합 단어 개수: 2, 자카드 유사도= 2/12 = 0.16666666666\n",
      "'딥러닝/딥러닝을 이용한 자연어처리 입문' Related Articles\n",
      "DESIGN BY TISTORY 관리자\n",
      "티스토리툴바\n",
      "-----------------------------------\n",
      "{'title': 'Python 자연어 처리 백터의 유사도 (코사인, 유클리드, 자카드) : 네이버 블로그', 'source': 'https://m.blog.naver.com/yug311861/222965948490', 'score': 0.9492, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "블로그\n",
      "카테고리 이동\n",
      "꾹의 개발노트\n",
      "Python 자연어 처리 백터의 유사도 (코사인, 유클리드, 자카드)\n",
      "2022. 12. 26. 13:30\n",
      "Python 자연어 처리\n",
      "백터의 유사도 (코사인, 유클리드, 자카드)\n",
      "© cdr6934, 출처 Unsplash\n",
      "오랜만에 파이썬 자연어관련 블로그 포스팅을 합니다.\n",
      "포스팅에 앞 서 모두들 크리스마스는 재미있게 보내셨는지요.\n",
      "저는 아내와 재미있는 시간을 보냈습니다.\n",
      "내년 소원도 같이 빌고 즐겁게 시간을 보냈어요 슝슝\n",
      "벌써 12월의 마지막 주라니 말도 안돼 2022년 정말 즐거운 한 해 였는데 기분 좋은 일이 더 많은 한 해였습니다.\n",
      "오늘 하루도 회사에서 새로운 다짐을 하고 있어요!\n",
      "본격적으로 백터의 유사도란?\n",
      "우리는 이전 포스팅에서\n",
      "수치화 유사도에 대해서 배웠습니다 Bow TF-IDF Word2Vec이 대표적인 수치화 방법이 였습니다.\n",
      "오늘은 수치화된 결과를 백터 유사도를 활용한 방법론에 대해서 알아보는 시간이 될 것 같습니다.\n",
      "​\n",
      "간략하게 정리하면 수치화된 결과를 토대로 문서의 유사도를 분석하는 방법입니다!\n",
      "대표적은 코사인\n",
      "그 외 유클리드, 자카드가 있습니다!\n",
      "그럼 하나하나 체크해보겠습니다!\n",
      "Python 코사인 유사도\n",
      "문서의 단어나 문장을 수치화 한 갚을 토대로 구하는 방법이며\n",
      "Numpy - norm (놈)을 활용하여 간단하게 테스트가 가능합니다!\n",
      "공식은 위와 같다고 합니다!\n",
      "자연어 처리 코사인 유사도는 위와 같이 두 데이터를 비교하여 각도를 기준으로 -1 0 1이 표현됩니다.\n",
      "당연히 1에 가까울 수록 유사하다는 뜻이며 서로 직각을 이루면 0의 형태를 지닙니다!\n",
      "https://wikidocs.net/24603\n",
      "​\n",
      "해당 내용을 토대로 보면 공식을 만들때 numpy의 두 가지 메서드를 사용하여 구합니다.\n",
      "또한 문서의 데이터는 수치화된 데이터이며 패딩된 데이터를 활용하는게 좋다고 보이네요!\n",
      "직접 테스트를 해보시면 될 것 같습니다!\n",
      "Python 유클리드, 자카드 유사도\n",
      "활용이 그렇게 많은 방법은 아니지만 유클리드, 자카드에 대해서도 가볍게 알아보자!\n",
      "​\n",
      "유클리드 기법\n",
      "기준이 되는 수치화된 문서 데이터가 하나 있어야합니다.\n",
      "해당 문서를 기준으로 다른 수치화된 데이터와 비교를 하는 방법론 입니다!\n",
      "(가장 중요한 것은 해당 문서들은 수치화된 데이터의 길이가 동일해야 합니다!\n",
      "패딩 기법을 같이 섞어서 만들어줘야한다는 뜻)\n",
      "공식은 이렇습니다 그냥 보고만 넘겨주세요!\n",
      "자카드 기법\n",
      "두 데이터의 합집합 데이터와 교집합 데이터를 활용하여 나누어주는 자연어 유사도 처리 기법\n",
      "합집합을 활용하요 교집합의 비율을 구해주는 기법이며 1에 가까울수록 유사하다는 것!\n",
      "​\n",
      "두 기법은 간단하게 이론만 이해하고 넘어가도 괜찮을 것으로 보입니다!\n",
      "미리 데이터셋이 뭐가 있는 지 알고 싶다면\n",
      "Download Open Datasets on 1000s of Projects + Share Projects on One Platform. Explore Popular Topics Like Government, Sports, Medicine, Fintech, Food, More. Flexible Data Ingestion.\n",
      "www.kaggle.com\n",
      "해당 사이트에서 자료를 참고해보세요!\n",
      "​\n",
      "파이썬(Python)에 사람들이 올려준 데이터셋을 활용하여 코사인 유사도 샘플 테스트는\n",
      "내일 같이 테스트해보겠습니다.\n",
      "오늘도 자연어 처리에 대해서 읽어주셔서 감사합니다.\n",
      "날씨가 많이 추워졌는데 따뜻하게 하고 겨울 보내세요!\n",
      "참고 문헌/자료\n",
      "https://wikidocs.net/book/2155\n",
      "#파이썬 #Python #백터의유사도 #코사인 #유클리드 #자카드\n",
      "배우고 노력하는 개발자\n",
      "모름은 배움의 시작\n",
      "@꾹\n",
      "yug311861@naver.com\n",
      "in.naver.com/programer\n",
      "이 블로그\n",
      "Py 자연어\n",
      "카테고리 글\n",
      "카테고리\n",
      "이 블로그\n",
      "Py 자연어\n",
      "카테고리 글\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.94913, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)\n",
      "What is Vector Similarity Search?\n",
      "Vector similarity search is a fundamental technique in machine learning, enabling efficient data retrieval and precise pattern recognition. It p\n",
      "벡터 유사도 검색은 무엇인가요? / What is Vector Similarity Search?\n",
      "Akruti Acharya • June 12, 2023 • 5 min read\n",
      "벡터 유사도 검색(Vector Similarity Search)은 머신러닝의 기본적인 기법으로, 효율적인 데이터 검색과 정확한 패턴 인식을 가능하게 합니다. 추천 시스템, 이미지 검색, 자연어 처리(NLP) 및 기타 어플리케이션 등에서 중추적인 역할을 수행하여 사용자 경험을 개선하고 데이터 기반의 의사결정을 주도합니다.\n",
      "Vector similarity search is a fundamental technique in machine learning, enabling efficient data retrieval and precise pattern recognition. It plays a pivotal role in recommendation systems, image search, NLP, and other applications, improving user experiences and driving data-driven decision-making.\n",
      "최인접 이웃 검색(Nearest Neighbor Search)이라고도 하는 벡터 유사도 검색은 고차원의 공간에서 유사한 벡터 또는 데이터 포인트를 찾는데 사용하는 방법입니다. 벡터 유사도 검색은 머신러닝 및 정보검색, 컴퓨터 비전, 추천 시스템 등의 다양한 영역에서 주로 사용됩니다.\n",
      "Vector similarity search, also known as nearest neighbor search, is a method used to find similar vectors or data points in a high-dimensional space. It is commonly used in various domains, such as machine learning, information retrieval, computer vision, and recommendation systems.\n",
      "벡터 유사도 검색의 기본 개념은 데이터 포인트를 다차원 공간에서 벡터로 표현하는 것으로, 각 차원은 특정 기능이나 속성에 해당합니다. 예를들어, 추천 시스템에서 사용자의 선호도를 벡터로 표현할 수 있으며, 이 때 각 요소(element)는 특정 항목 또는 카테고리에 대한 사용자의 선호도를 나타냅니다. 마찬가지로, 이미지 검색에서 이미지는 컬러 히스토그램(histogram)이나 이미지 임베딩과 같이 이미지에서 추출한 특징의 벡터로 표현할 수 있습니다.\n",
      "The fundamental idea behind vector similarity search is to represent data points as vectors in a multi-dimensional space, where each dimension corresponds to a specific feature or attribute. For example, in a recommendation system, a user’s preferences can be represented as a vector, with each element indicating the user’s preferences for a particular item or category. Similarly, in image search, an image can be represented as a vector of features extracted from the image, such as color histograms or image embeddings.\n",
      "이번 글에서는 머신러닝 분야에서의 벡터 유사도 검색의 중요성과 AI 임베딩이 이를 개선하는데 어떻게 도움이 되는지에 대해 설명합니다. 다음 내용들을 다룹니다:\n",
      "In this blog, we’ll discuss the importance of vector similarity search in machine learning and how AI embeddings can help improve it. We will cover the:\n",
      "벡터 유사도 검색이 해결하는 문제는 무엇인가요? / What Problem is Vector Similarity Search Solving?\n",
      "벡터 유사도 검색은 대규모 데이터셋, 특히 고차원 공간에서 유사한 항목이나 데이터 포인트를 효율적으로 검색해야 하는 문제를 해결합니다. 다음은 벡터 유사도 검색으로 해결할 수 있는 몇 가지 문제들입니다:\n",
      "Vector similarity search addresses the challenge of efficiently searching for similar items or data points in large datasets, particularly in high-dimensional spaces. Here are some of the problems vector similarity search addresses:\n",
      "차원의 저주 / Curse of Dimensionality\n",
      "고차원 공간에서는 데이터의 희소성이 기하급수적으로 증가하여 유사한 항목과 그렇지 않은 항목을 구분하기 어렵습니다. 예를 들어, 512x512 픽셀의 해상도를 갖는 이미지에는 262,144차원의 원시 데이터가 포함되어 있습니다. 이러한 고차원의 원시 데이터(raw data)로 직접 작업하는 것은 연산 비용이 많이 들고 비효율적입니다.\n",
      "In high-dimensional spaces, the sparsity of data increases exponentially, making it difficult to distinguish similar items from dissimilar ones. For example, an image of resolution 512⨯512 pixels contains raw data of 262,144 dimensions. Working directly with the raw data in its high-dimensional form can be computationally expensive and inefficient.\n",
      "기존의 검색 방법으로는 이러한 차원의 저주(Curse of Dimensionality) 문제를 효율적으로 처리하기 어렵습니다. 임베딩을 사용하면 이러한 차원을 훨씬 더 낮은, 1024차원 등으로 줄일 수 있습니다. 이러한 차원 축소는 저장 공간을 줄이고, 계산 속도를 향상시키며, 중요한 정보(feature)는 보존하면서 불필요한 정보를 삭제하는 등의 이점을 얻을 수 있습니다.\n",
      "Traditional search methods struggle to handle this curse of dimensionality efficiently. Using embeddings can reduce these dimensions to 1,024, which is significantly lower. This reduction offers advantages such as decreased storage space, faster computations, and elimination of irrelevant information while preserving important features.\n",
      "키워드 기반 검색의 비효율성 / Ineffective keyword-based search\n",
      "키워드 기반 검색이나 일치 검색(exact match)과 같은 기존의 검색 방법은 명시적 키워드가 아닌, 다차원적인 특징(characteristic)을 기반으로 유사도(similarity)를 찾는 시나리오에는 적합하지 않습니다.\n",
      "Traditional search methods, such as keyword-based search or exact matching, are not suitable for scenarios where similarity is based on multi-dimensional characteristics rather than explicit keywords.\n",
      "확장성 / Scalability\n",
      "대규모 데이터셋을 검색하는데는 연산 비용과 시간이 많이 소요될 수 있습니다. 벡터 유사도 검색 알고리즘과 데이터 구조는 검색 공간을 효율적으로 정리(prune)하여, 거리 연산 횟수를 줄이고 유사한 항목들을 더 빠르게 검색할 수 있는 방법을 제공합니다.\n",
      "Searching through large datasets can be computationally expensive and time-consuming. Vector similarity search algorithms and data structures provide efficient ways to prune the search space, reducing the number of distance computations required and enabling faster retrieval of similar items.\n",
      "비정형 또는 반정형 데이터 / Unstructured or Semi-Structured Data\n",
      "벡터 유사도 검색은 이미지, 동영상, 텍스트 문서 또는 센서값 등과 같은 비정형 또는 반정형 데이터를 다룰 때 매우 중요합니다. 이러한 데이터 타입은 자연스럽게 벡토 또는 특징 벡터(feature vector)로 표현되며, 거리 값(metric)을 사용하면 유사도를 더 잘 파악할 수 있습니다.\n",
      "Vector similarity search is crucial when dealing with unstructured or semi-structured data types, such as images, videos, text documents, or sensor readings. These data types are naturally represented as vectors or feature vectors, and their similarity is better captured using distance metrics.\n",
      "정형 데이터와 비정형 데이터의 차이를 보여주는 그림. 출처 / Illustration showing the difference between structured and unstructured data. Source\n",
      "벡터 유사도 검색은 이러한 문제들을 해결함으로써 추천 시스템, 콘텐츠 기반 검색, 이상 징후 감지(anomaly detection), 클러스터링 등과 같은 다양한 머신러닝 및 데이터 분석 작업의 효율성과 효과를 향상시킵니다.\n",
      "By addressing these problems, vector similarity search enhances the efficiency and effectiveness of various machine learning and data analysis tasks, contributing to advancements in recommendation systems, content-based search, anomaly detection, and clustering.\n",
      "벡터 유사도는 어떻게 동작하나요? / How Does Vector Similarity Work?\n",
      "벡터 유사도 검색은 3가지 주요한 구성 요소를 포함하고 있습니다:\n",
      "Vector similarity search involves three key components:\n",
      "벡터 임베딩 / Vector Embeddings\n",
      "벡터 임베딩은 필수적인 특징(essential feature)과 패턴을 갖는 저차원의 데이터로, 유사도 검색 및 머신러닝과 같은 작업(task)에서 효율적인 연산과 분석을 가능하게 합니다. 이는 데이터에서 의미있는 특징(feature)을 추출함으로써 가능합니다. 예를 들어, 자연어 처리(NLP) 분야에서 Word2Vec이나 GloVe와 같은 단어 임베딩 기술은 단어나 문서를 의미 관계(semantic relationship)를 포착하는 조밀하고(dense), 저차원적인 벡터로 변환합니다. 컴퓨터 비전(CV) 분야에서는 합성곱 신경망(CNN; Convolutional Neural Network)과 같은 이미지 임베딩 방법으로 이미지로부터 시각적 특징(visual feature)을 추출하여 고차원 벡터로 표현합니다.\n",
      "Vector embeddings are lower-dimensional representations of data that capture essential features and patterns, enabling efficient computation and analysis in tasks like similarity searches and machine learning. This is achieved by extracting meaningful features from the data. For example, in natural language processing, word embedding techniques like Word2Vec or GloVe transform words or documents into dense, low-dimensional vectors that capture semantic relationships. In computer vision, image embedding methods such as convolutional neural networks (CNNs) extract visual features from images and represent them as high-dimensional vectors.\n",
      "Word2Vec을 사용하여 생성한 벡터 임베딩을 보여주는 예시. 출처 / Example showing vector embeddings generated using Word2Vec. Source\n",
      "유사도 점수 계산 / Similarity Score Computation\n",
      "데이터 포인트가 벡터로 표현되면 유사도 점수를 계산하여 두 벡터 간의 유사도를 정량화(quantify)합니다. 유사도 계산에 사용하는 일반적인 거리 기준(metric)에는 유클리드 거리(Euclidean distance), 멘하탄 거리(Manhattan distance), 코사인 유사도(cosine similarity) 등이 있습니다.\n",
      "Once the data points are represented as vectors, a similarity score is computed to quantify the similarity between two vectors. Common distance metrics used for similarity computation include Euclidean distance, Manhattan distance, and cosine similarity.\n",
      "유클리드 거리는 공간 내의 두 점간의 직선 거리를 측정하고, 맨하탄 거리는 각 차원의 차이의 절대값의 합을 계산하며, 코사인 유사도는 두 벡터 사이 각도의 코사인 값을 측정합니다. 거리 기준(metric)을 고르는 기준은 데이터의 특성과 사용 중인 어플리케이션에 따라 달라집니다.\n",
      "Euclidean distance measures the straight-line distance between two points in space, Manhattan distance calculates the sum of the absolute differences between corresponding dimensions, and cosine similarity measures the cosine of the angle between two vectors. The choice of distance metric depends on the characteristics of the data and the application at hand.\n",
      "최인접 이웃(NN) 알고리즘 / NN Algorithms\n",
      "최인접 이웃(NN; nearest neighbor) 알고리즘은 주어진 질의 벡터(query vector)의 가장 가까운 이웃(들)을 효율적으로 검색하는데 사용합니다.이 알고리즘은 검색 속도를 크게 향상시키기 위해 정확도를 다소 희생(trade-off)합니다. 대규모의 유사도 작업을 위해 여러 ANN(Approximate Nearest Neighbor; 근사 최인접 이웃) 알고리즘이 개발되었습니다:\n",
      "Nearest neighbor (NN) algorithms are employed to efficiently search for the nearest neighbors of a given query vector. These algorithms trade off a small amount of accuracy for significantly improved search speed. Several ANN algorithms have been developed to handle large-scale similarity tasks:\n",
      "kNN 알고리즘은 데이터셋의 모든 벡터들과의 거리를 비교하여 질의 벡터(query vector)로부터 가장 가까운 이웃 k개를 검색합니다. 무차별 검색(brute-force search)을 하거나 k-d 트리 또는 볼 트리(ball tree)와 같은 자료구조를 사용하여 최적화할 수 있습니다.\n",
      "The kNN algorithm searches for the k nearest neighbors of a query vector by comparing distances to all vectors in the dataset. It can be implemented using brute-force search or optimized with data structures like k-d trees or ball trees.\n",
      "나이브 k-인접 이웃(kNN) 알고리즘은 연산 비용이 많이 들고 복잡도가 O(n^2) 이며, 여기서 n 은 데이터 포인트의 개수입니다. 이는 각각 데이터 포인트 쌍 사이의 거리를 계산해야 하므로 n*(n-1)/2 번의 거리 계산을 해야 하기 때문입니다.\n",
      "Naive k-nearest neighbors (kNN) algorithm is computationally expensive, with a complexity of O(n^2), where n is the number of data points. This is because it requires calculating the distances between each pair of data points, resulting in n*(n-1)/2 distance calculations.\n",
      "kNN 알고리즘. 출처 / kNN algorithm.Source\n",
      "SPTAG는 벡터를 계층 구조로 구성하는 효율적인 그래프 기반의 인덱싱 구조입니다. 그래프 분할 기법을 사용하여 벡터를 영역으로 나누기 때문에 가장 가까운 이웃을 더 빠르게 검색할 수 있습니다.\n",
      "SPTAG is an efficient graph-based indexing structure that organizes vectors into a hierarchical structure. It uses graph partitioning techniques to divide the vectors into regions, allowing for a faster nearest-neighbor search.\n",
      "HNSW는 그래프 기반의 알고리즘으로, 효율적인 검색을 위한 방법으로 벡터를 연결하여 계층적 그래프로 구성하는 방법입니다. 무작위 추출(randomization)과 지역 탐색(local exploration)을 조합하여 탐색 가능한 그래프 구조를 구축합니다.\n",
      "HNSW is a graph-based algorithm that constructs a hierarchical graph by connecting vectors in a way that facilitates efficient search. It uses a combination of randomization and local exploration to build a navigable graph structure.\n",
      "HNSW 그림. 출처 / Illustration of HNSW. Source\n",
      "Faiss는 Facebook에서 밀집 벡터(dense vector)의 효율적 검색과 클러스터링을 위해 개발한 라이브러리입니다. 이 라이브러리는 정확도와 속도 간의 다양한 트레이드-오프(trade-off)에 최적화된 반전된 인덱스(inverted indices), 벡터 양자화(product quantization), 근사적인 거리 계산을 한 반전된 파일(IVFADC) 등의 다양한 인덱스 구조를 제공합니다.\n",
      "Faiss is a library developed by Facebook for efficient similarity search and clustering of dense vectors. It provides various indexing structures, such as inverted indices, product quantization, and IVFADC (Inverted File with Approximate Distance Calculation), which are optimized for different trade-offs between accuracy and speed.\n",
      "이러한 ANN 알고리즘들은 벡터 공간의 특성을 잘 살리고(leverage) 인덱싱 구조를 활용(explot)하여 검색을 빠르게 합니다. 가장 가까운 이웃을 포함할 가능성이 높은 영역(region)에 집중하여 검색 공간을 축소(reduce)함으로써 유사한 백터를 빠르게 찾을 수 있도록 합니다.\n",
      "These ANN algorithms leverage the characteristics of the vector space and exploit indexing structures to speed up the search process. They reduce the search space by focusing on regions likely to contain nearest neighbors, allowing for fast retrieval of similar vectors.\n",
      "벡터 유사도 검색의 사용 사례 / Use cases for Vector Similarity Search\n",
      "백터 유사도 검색은 다양한 분야(domain)에서 수많은 예제와 사용 사례가 있습니다. 여기에서는 몇 가지 대표적인 사례를 살펴보겠습니다:\n",
      "Vector similarity search has numerous examples and use cases across various domains. Here are some prominent examples:\n",
      "추천 시스템 / Recommendation Systems\n",
      "벡터 유사도 검색은 추천 시스템에서 중요한 역할을 합니다. 사용자와 항목(item)을 벡터로 표현함으로써, 유사한 사용자나 항목(item)들을 찾아내어 개인화된 추천을 가능하게 합니다. 예를 들어, 온라인 상거래(e-commerce) 분야에서는 사용자 선호도에 따라 제품을 추천하거나 협업 필터링(CF; collaborative filtering)으로 유사한 사용자를 찾는데 사용할 수 있습니다.\n",
      "Vector similarity search plays a crucial role in recommendation systems. By representing users and items as vectors, similarity search helps find similar users or items, enabling personalized recommendations. For example, in e-commerce, it can be used to recommend products based on user preferences or to find similar users for collaborative filtering.\n",
      "이미지 및 비디오 검색 / Image and Video Search\n",
      "벡터 유사도 검색은 이미지 및 비디오 검색 어플리케이션에서 널리 사용합니다. 이미지나 비디오를 고차원의 특징 벡터(feature vector)로 표현함으로써, 시각적으로 유사한 콘텐츠를 찾는데 도움을 줍니다. 역방향 이미지 검색, 콘텐츠 기반 추천, 동영상 검색과 같은 작업들에 사용합니다.\n",
      "Vector similarity search is widely used in image and video search applications. By representing images or videos as high-dimensional feature vectors, similarity search helps locate visually similar content. It supports tasks such as reverse image search, content-based recommendation, and video retrieval.\n",
      "자연어 처리(NLP) / Natural Language Processing (NLP)\n",
      "자연어 처리 분야에서 벡터 유사도 검색은 문서 유사도, 의미 검색(semantic search), 단어 임베딩 같은 작업에 사용합니다. Word2Vec이나 GloVe 같은 단어 임베딩 기법은 단어 또는 문서를 벡터로 변환하여 의미론적 뜻(semantic meaning)에 따라 유사한 문서 또는 단어를 효율적으로 검색할 수 있습니다.\n",
      "In NLP, vector similarity search is employed for tasks like document similarity, semantic search, and word embeddings. Word embedding techniques like Word2Vec or GloVe transform words or documents into vectors, enabling efficient search for similar documents or words based on their semantic meaning.\n",
      "이상 징후 탐지 / Anomaly Detection\n",
      "벡터 유사도 검색은 이상 징후 탐지 어플리케이션에도 활용합니다. 데이터 포인트의 벡터를 정규 분포 또는 예상 분포와 비교하여 유사한 데이터를 식별할 수 있습니다. (전체 집단의) 다수(majority)로부터의 차이(deviations)로 이상 징후를 감지(consider)하여 사기 거래, 네트워크 침입 및 장비의 이상을 탐지하는데 도움을 줄 수 있습니다.\n",
      "Vector similarity search is utilized in anomaly detection applications. By comparing the vectors of data points to a normal or expected distribution, similar vectors can be identified. Deviations from the majority can be considered anomalies, aiding in detecting fraudulent transactions, network intrusions, or equipment failures.\n",
      "클러스터링 / Clustering\n",
      "클러스터링 알고리즘은 종종 벡터 유사도 검색에 의존하여 유사한 데이터 포인트를 함께 그룹으로 묶습니다. 클러스터링 알고리즘은 가장 가까운 이웃 또는 가장 유사한 벡터를 식별함으로써, 데이터를 의미있는 그룹들로 효율적으로 분할(partition)할 수 있습니다. 이는 고객 세분화, 이미지 세분화(image segmentation) 및 유사한 인스턴스들끼리 묶어야 하는(grouping) 모든 종류의 작업에 유용합니다.\n",
      "Clustering algorithms often rely on vector similarity search to group similar data points together. By identifying nearest neighbors or most similar vectors, clustering algorithms can efficiently partition data into meaningful groups. This is beneficial in customer segmentation, image segmentation, or any task that involves grouping similar instances.\n",
      "Encord Annotate를 사용한 이미지 세분화 / Image Segmentation using Encord Annotate\n",
      "게놈 시퀀싱 / Genome Sequencing\n",
      "유전체학(genomics; 게놈학)에서, 벡토 유사도 검색은 DNA 서열을 분석하는데 도움이 됩니다. DNA 서열을 벡터로 표현하고 유사성 검색 알고리즘을 사용함으로써 연구자들은 효율적으로 순서(sequence)를 비교하고, 유전적 유사성을 식별하고, 유전적 변이(variation)를 발견할 수 있습니다.\n",
      "In genomics, vector similarity search helps analyze DNA sequences. By representing DNA sequences as vectors and employing similarity search algorithms, researchers can efficiently compare sequences, identify genetic similarities, and discover genetic variations.\n",
      "소셜 네트워크 분석 / Social Network Analysis\n",
      "벡터 유사도 검색은 소셜 네트워크 분석 분야에서 사회적 연결이나 행동을 기반으로 유사한 사람들을 찾는데 사용합니다. 개개인을 벡터로 표현한 뒤 그들의 유사도를 측정함으로써, 영향력있는 사용자를 찾거나 소그룹(community) 찾기, 또는 잠재적 연결(potential connection)을 제안하는 작업에 도움을 줄 수 있습니다.\n",
      "Vector similarity search is used in social network analysis to find similar individuals based on their social connections or behavior. By representing individuals as vectors and measuring their similarity, it assists in tasks such as finding influential users, detecting communities, or suggesting potential connections.\n",
      "콘텐츠 필터링 및 검색 / Content Filtering and Search\n",
      "벡터 유사도 검색은 콘텐츠 필터링 및 검색 어플리케이션에서 사용합니다. 유사도 검색은 문서, 기사 또는 웹 페이지를 벡터로 표현함으로써 질의 벡터(query vector)와의 유사도를 기반으로 관련 콘텐츠를 효율적으로 검색할 수 있게 해줍니다. 이는 뉴스 추천, 콘텐츠 필터링 및 검색엔진 등에서 유용합니다.\n",
      "Vector similarity search is employed in content filtering and search applications. By representing documents, articles, or web pages as vectors, similarity search enables efficient retrieval of relevant content based on their similarity to a query vector. This is useful in news recommendations, content filtering, or search engines.\n",
      "위에서 언급한 예시들은 다양한 분야(domain)에서 벡터 유사도 검색의 다양한 활용도(versatility)과 중요성(importance)를 보여주고 있습니다. 고차원 벡터와 유사도 기준(metric)의 기능을 활용(harness)함으로써, 벡터 유사도 검색으로 효율적인 정보 검색과 패턴 인식, 데이터 탐색 등을 용이하게 할 수 있습니다. 하지만, 현실 세계에서 벡터 유사도 검색을 적용할 때 마주칠 수 있는 문제들을 아는 것이 중요합니다.\n",
      "The examples mentioned above highlight the versatility and importance of vector similarity search across different domains. By harnessing the capabilities of high-dimensional vectors and similarity metrics, this approach facilitates efficient information retrieval, pattern recognition, and data exploration. However, it is essential to acknowledge the challenges associated with employing vector similarity search in real-world applications.\n",
      "벡터 유사도 검색의 걸림돌 / Vector Similarity Search Challenges\n",
      "벡터 유사도 검색은 많은 이점을 제공하지만, 현실 세계에 적용하기 위해서는 몇 가지 걸림돌(challenge)들을 해결해야 합니다. 주요한 몇 가지 걸림돌들은 다음과 같습니다:\n",
      "While vector similarity search offers significant benefits, it also presents certain challenges in real-world applications. Some of the key challenges include:\n",
      "고차원 데이터 / High-dimensional Data\n",
      "데이터의 차원이 증가함에 따라 차원의 저주(curse of dimensionality)는 주요한 걸림돌이 됩니다. 고차원 공간에서는 데이터의 희소성(sparsity)가 증가하게 되어 의미있는 유사도를 식별하기가 어렵습니다. 이로 인해 검색 성능이 저하되고 계산의 복잡도가 높아질 수 있습니다.\n",
      "As the dimensionality of the data increases, the curse of dimensionality becomes a significant challenge. In high-dimensional spaces, the sparsity of data increases, making it difficult to discern meaningful similarities. This can result in degraded search performance and increased computational complexity.\n",
      "확장성 / Scalability\n",
      "대규모 데이터셋을 검색하는데는 연산 비용과 시간이 많이 들 수 있습니다. 데이터셋의 크기가 커지면 비교해야 할 벡터의 수가 증가하기 때문에, 검색 프로세스는 더욱 어려워집니다. 벡터 유사도 검색과 관련한 확장성 문제를 해결하기 위해서는 효율적인 인덱싱 구조와 알고리즘이 필요합니다.\n",
      "Searching through large-scale datasets can be computationally expensive and time-consuming. As the dataset size grows, the search process becomes more challenging due to the increased number of vectors to compare. Efficient indexing structures and algorithms are necessary to handle the scalability issues associated with vector similarity search.\n",
      "거리 측정 기준 고르기 / Choice of Distance Metric\n",
      "벡터 유사도 검색에서는 적절한 거리 측정 지표(metric)를 선택하는 것이 중요합니다. 데이터의 특성과 적용 분야에 따라 서로 다른 거리 측정 지표는 서로 다른 결과를 야기(yield)할 수 있습니다. 원하는 유사도의 개념(notion of similarity)에 맞는 올바른 지표를 고르는 일은 결코 쉬운(non-trivial) 일이 아니므로 신중하게 고려해야 합니다.\n",
      "The selection of an appropriate distance metric is crucial in vector similarity search. Different distance metrics may yield varying results depending on the nature of the data and the application domain. Choosing the right metric that captures the desired notion of similarity is a non-trivial task and requires careful consideration.\n",
      "인덱싱 및 저장소 요구사항 파악하기 / Indexing and Storage Requirements\n",
      "고차원 공간에서 빠른 검색을 위해서는 효율적인 인덱싱 구조가 필수적입니다. 이러한 인덱스를 구축 및 유지하려면 스토리지 오버헤드가 발생할뿐만 아니라 연산 자원도 필요로 합니다. 인덱싱의 효율성과 스토리지의 요구 사항 사이의 균형을 맞추는 것(trade-off)은 벡터 유사도 검색에 있어서 어려운 과제입니다.\n",
      "Efficient indexing structures are essential to support fast search operations in high-dimensional spaces. Constructing and maintaining these indexes can incur storage overhead and necessitate computational resources. Balancing the trade-off between indexing efficiency and storage requirements is a challenge in vector similarity search.\n",
      "정확도와 효율성 사이의 균형 맞추기 / The trade-off between Accuracy and Efficiency\n",
      "많은 근사 최인접 이웃(ANN; Approximate Nearest Neighbor) 알고리즘은 더 빠른 검색 성능을 위해 정확도를 어느 정도 희생합니다. 검색의 정확도와 계산 효율성 사이의 적절한 균형을 맞추는 것은 매우 중요하며, 이는 어플리케이션의 요구사항과 제약 조건에 따라 달라집니다.\n",
      "Many approximate nearest neighbor (ANN) algorithms sacrifice a certain degree of accuracy to achieve faster search performance. Striking the right balance between search accuracy and computational efficiency is crucial and depends on the specific application requirements and constraints.\n",
      "데이터 분포 및 쏠림 / Data Distribution and Skewness\n",
      "데이터 분포(distribution)와 쏠림(skewness)은 벡터 유사도 검색 알고리즘의 효율에 영향을 미칠 수 있습니다. 데이터의 분포가 균일하지 않거나(non-uniform), 데이터가 불균형(imbalance)하면 검색 결과가 편향(bias)되거나 질의 성능이 최적화되지 않을 수(suboptimal) 있습니다. 데이터의 쏠림을 처리하고 다양한 데이터 분포에서도 견고(robust)한 알고리즘을 설계하는 것은 어려운 과제입니다.\n",
      "The distribution and skewness of the data can impact the effectiveness of vector similarity search algorithms. Non-uniform data distributions or imbalanced data can lead to biased search results or suboptimal query performance. Handling data skewness and designing algorithms that are robust to varying data distributions pose challenges.\n",
      "결과의 설명 가능성 / Interpretability of Results\n",
      "벡터 유사도 검색은 주로 수학적 유사도를 측정한 값을 기반으로 유사한 벡터를 식별하는데 중점을 둡니다. 이는 많은 어플리케이션에서 효과적일 수 있지만, 결과를 설명하는 것(interpretability)은 어려울 수 있습니다. 두 벡터가 유사한 것으로 판단(consider)한 이유를 이해하고, 결과에서 의미있는 인사이트를 도출하려면 추가적인 후처리(post-processing)와 분야별 전문지식(domain-specific knowledge)이 필요할 수 있습니다.\n",
      "Vector similarity search primarily focuses on identifying similar vectors based on mathematical similarity measures. While this can be effective for many applications, the interpretability of the results can sometimes be challenging. Understanding why two vectors are considered similar and extracting meaningful insights from the results may require additional post-processing and domain-specific knowledge.\n",
      "벡터 유사도 검색 문제의 해결방법 / How to Solve Vector Similarity Search Challenges\n",
      "다음은 고차원 데이터 처리, 거리 측정 기준 선택, 인덱싱 및 저장소 요구사항 등, 위에서 나열한 문제들을 해결하는데 사용할 수 있는 몇 가지 접근 방식입니다.\n",
      "Here are some approaches you can use to solve the challenges listed above, including handling high-dimensional data, the choice of distance metrics, and indexing and storage requirements.\n",
      "고차원 데이터 / High-Dimensional Data\n",
      "주성분 분석(PCA) 또는 t-SNE 등의 기법들을 사용하여 의미있는 유사도를 보존하면서 데이터의 차원을 줄입니다.\n",
      "Apply techniques like Principal Component Analysis (PCA) or t-SNE to reduce the dimensionality of the data while preserving meaningful similarities.\n",
      "특징(feature) 선택: 가장 관련이 높은 특징(feature)을 고르고 유지하여 차원을 줄이고 차원의 저주를 완화합니다.\n",
      "Feature selection: Identify and retain only the most relevant features to reduce the dimensionality and mitigate the curse of dimensionality.\n",
      "데이터를 정규화(normalize)하거나 규모(scale)를 조절하여 특징(feature)의 크기(scale)가 유사도 계산에 끼치는 영향을 줄입니다. 이는 입력 데이터를 적절히 준비하고, 이상값(outlier)과 결측치(missing value)를 처리하며, 입력 데이터의 전반적인 품질을 개선하기 위한 것으로, PCA에서 특히 중요한 절차입니다.\n",
      "Normalize or scale the data to alleviate the impact of varying feature scales on similarity computations. This is especially important in PCA to ensure that the input data is appropriately prepared, outliers and missing values are handled, and the overall quality of the input data is improved.\n",
      "거리 측정 기준 고르기 / Choice of Distance Metric\n",
      "특정 분야(specific domain) 또는 어플리케이션의 요구사항에 맞는 거리 측정 기준(metric)을 설계하거나 선택하여 원하는 유사도의 개념(notion of similarity)을 보다 정확하게\n",
      "측정할 수 있도록 합니다.\n",
      "Design or select distance metrics that align with the specific domain or application requirements to capture the desired notion of similarity more accurately.\n",
      "데이터의 특성(characteristic)에 따라 유사도 측정을 동적(dynamically)으로 조정하는 적응형 또는 학습에 기반한 거리 측정 기준들을 찾아보세요.\n",
      "Explore adaptive or learning-based distance metrics that dynamically adjust the similarity measurement based on the characteristics of the data.\n",
      "인덱싱 및 저장소 요구사항 / Indexing and Storage Requirements\n",
      "적절한 인덱싱 구조와 압축 기법을 선택하여 인덱싱의 효율성과 저장소 요구사항 사이의 균형을 맞출 수 있도록 합니다.\n",
      "Balance the trade-off between indexing efficiency and storage requirements by selecting appropriate indexing structures and compression techniques.\n",
      "저장 공간과 검색의 정확도, 그리고 질의의 효율성 사이의 절충점을 찾을 수 있는 근사(approximate) 인덱싱 방식을 사용(adopt)해보세요.\n",
      "Adopt approximate indexing methods that provide trade-offs between storage space, search accuracy, and query efficiency.\n",
      "신경망 해싱 / Neural Hashing\n",
      "신경망 해싱은 벡터 유사도 검색의 문제를, 특히 정확도와 속도 측면에서 해결할 수 있는 기법입니다. 신경망 해싱을 활용(leverage)하여 고차원 벡터를 표현하는 간결한 바이너리 코드(binary code)를 생성하여, 효율적이고 정확한 유사도 검색을 가능케 합니다.\n",
      "Neural hashing is a technique that can be employed to tackle the challenges of vector similarity search, particularly in terms of accuracy and speed. It leverages neural networks to generate compact binary codes that represent high-dimensional vectors, enabling efficient and accurate similarity searches.\n",
      "신경망 해싱의 예시. 출처 / An example of neural network hashing. Source\n",
      "신경망 해싱은 다음 세가지 단계로 동작합니다:\n",
      "Neural hashing works in three phases:\n",
      "학습 단계에서는 신경망 모델이 고차원 벡터를 간결한 바이너리 코드로 변환하는 변환 함수(mapping function)를 학습합니다. 이러한 변환 함수는 원본 벡터들 간의 유사도를 보존하도록 합니다.\n",
      "In the training phase, a neural network model is trained to learn a mapping function that transforms high-dimensional vectors into compact binary codes. This mapping function aims to preserve the similarity relationships between the original vectors.\n",
      "신경망이 학습되면, 이를 사용하여 데이터셋 내의 벡터들을 가지고 바이너리 코드를 생성합니다. 각각의 벡터는 바이너리 코드로 인코딩(encode)되며, 이 때 바이너리 코드의 각 비트들은 특정한 특징(feature) 또는 벡터의 특징(characteristic)을 표현합니다.\n",
      "Once the neural network is trained, it is used to generate binary codes for the vectors in the dataset. Each vector is encoded as a binary code, where each bit represents a specific feature or characteristic of the vector.\n",
      "유사도 검색 단계에서는 원래의 고차원 벡터 대신 바이너리 코드들을 비교합니다. 이 때, 바이너리 코드들 간의 비교는 고차원 벡터들 간의 비교에 비해서 (유사도를) 계산하는 비용이 저렴하기 때문에 효율적으로 검색을 할 수 있습니다.\n",
      "During the similarity search phase, the binary codes are compared instead of the original high-dimensional vectors. This enables efficient search operations, as comparing binary codes is computationally inexpensive compared to comparing high-dimensional vectors.\n",
      "신경망 해싱은 간결한 바이너리 코드로 데이터의 차원을 줄임으로써 효율성을 높이고 대규모 데이터셋의 확장성을 향상시킵니다. 이는 신경망을 통해 근본적인(underlying) 유사도를 포착하여 정확하게 유사도를 보존하고, 정확한 유사도 검색을 용이하게 합니다.\n",
      "Neural hashing offers improved efficiency by reducing the dimensionality of data with compact binary codes, enhancing scalability for large-scale datasets. It ensures accurate similarity preservation by capturing underlying similarities through neural networks, facilitating precise similarity searches.\n",
      "신경망 해싱 기법은 저장소를 작게(compact)하여, 저장소 요구사항을 줄임으로써 보다 효율적인 데이터 검색을 가능케 합니다. 사용자는 바이너리 코드의 길이를 변경함으로써 검색의 정확도와 계산의 효율성 간의 균형을 유연하게 맞출 수 있습니다. 신경망 해싱은 텍스트와 이미지 등의 다양한 분야(domain)에 적용할 수 있어 다양한 어플리케이션에 다용도로 사용할 수 있습니다.\n",
      "The technique enables compact storage, reducing storage requirements for more efficient data retrieval. With flexibility in trade-offs, users can adjust binary code length to balance search accuracy and computational efficiency. Neural hashing is adaptable to various domains, such as text and images, making it versatile for different applications.\n",
      "컴퓨터 비전(CV) 분야에서의 벡터 유사도 검색 사용 사례 / How Vector Similarity Search can be used in Computer Vision\n",
      "벡터 유사도 검색은 다양한 컴퓨터 비전 어플리케이션에서 매우 중요하며, 효율적인 객체 탐지, 인식 및 검색을 가능하게 합니다. 이렇게 이미지를 고차원 특징 벡터로 표현함으로써, 벡터 유사도 검색 알고리즘은 대규모 데이터셋에서 유사한 이미지를 찾을(locate) 수 있게 하는 등, 여러가지 실질적인 이점을 제공합니다.\n",
      "Vector similarity search is crucial in various computer vision applications, enabling efficient object detection, recognition, and retrieval. By representing images as high-dimensional feature vectors, vector similarity search algorithms can locate similar images in large datasets, leading to several practical benefits.\n",
      "객체 탐지 / Object Detection\n",
      "벡터 유사도 검색은 관심있는 객체(object of interest)가 포함된 유사한 이미지를 찾음으로써 객체 탐지에 도움을 줄 수 있습니다. 예를 들어, 시스템은 사전에 학습(pre-trained)된 딥러닝 모델을 사용하여 이미지에서 특징 벡터(feature vector)를 추출(extract)할 수 있습니다. 이렇게 추출된 벡터들은 벡터 유사도 검색 비법을 사용하여 인덱싱합니다. 실행 시(runtime)에 새로운 이미지가 입력되면, 그 이미지의 특징 벡터를 인덱싱된 벡터들과의 비교를 통해 유사한 객체를 갖는 이미지들을 찾을(identify) 수 있습니다. 이렇게 실시간으로 물체를 감지하고 시각적 검색(visual search)과 같은 어플리케이션을 지원할 수 있습니다.\n",
      "Vector similarity search can aid in object detection by finding similar images that contain objects of interest. For example, a system can use pre-trained deep-learning models to extract feature vectors from images. These vectors can then be indexed using vector similarity search techniques. During runtime, when a new image is provided, its feature vector is compared to the indexed vectors to identify images with similar objects. This can assist in detecting objects in real-time and supporting applications like visual search.\n",
      "Encord에서 제공하는 객체 검색과 그 사용 사례에 대한 가이드를 읽어보세요..\n",
      "Read our guide on object detection and its use cases to find out more.\n",
      "이미지 검색 / Image Retrieval\n",
      "벡터 유사도 검색은 효율적인 콘텐츠 기반의 이미지 검색을 가능하게 합니다. 이미지를 고차원의 특징 벡터로 변환함으로써 시각적으로 유사한 콘텐츠가 있는 이미지들을 빠르게 식별할 수 있습니다. 이는 사용자가 이미지를 입력하고 데이터베이스에서 시각적으로 유사한 이미지를 검색하는 역방향 이미지 검색(reverse image search)과 같은 어플리케이션에서 특히 유용합니다. 이러한 기능은 이미지 검색 엔진, 이미지에 기반한 제품 추천, 구조화(organization)를 위한 이미지 클러스터링 등의 어플리케이션 등에서 사용할 수 있습니다.\n",
      "Vector similarity search allows for efficient content-based image retrieval. By converting images into high-dimensional feature vectors, images with similar visual content can be identified quickly. This is particularly useful in applications such as reverse image search, where users input an image and retrieve visually similar images from a database. It enables applications like image search engines, product recommendations based on images, and image clustering for organizations.\n",
      "이미지 인식 / Image Recognition\n",
      "벡터 유사도 검색은 입력 이미지의 특징 벡터를 이미 갖고있는(known) 이미지들의 인덱싱된 벡터들과 비교하여 이미지 인식 작업을 용이하게 합니다. 이러한 기능은 이미지 분류(image classification) 및 이미지 유사도 순위 지정(image similarity ranking) 등에 사용합니다.\n",
      "Vector similarity search facilitates image recognition tasks by comparing feature vectors of input images with indexed vectors of known images. This aids in tasks like image classification and image similarity ranking.\n",
      "예를 들어, 이미지 인식 시스템에서는 질의 이미지(query image)의 특징 벡터를 알려진 분류(class) 또는 이미 인덱싱된 이미지들의 벡터와 비교하여 가장 유사한 분류(class) 또는 이미지를 찾습니다. 이러한 접근 방식은 얼굴 인식이나 장면 인식, 이미지 분류(image categorization)과 같은 어플리케이션에 사용됩니다.\n",
      "For example, in image recognition systems, a query image's feature vector is compared to the indexed vectors of known classes or images to identify the most similar class or image. This approach is employed in applications like face recognition, scene recognition, and image categorization.\n",
      "이미지 세분화 / Image Segmentation\n",
      "벡터 유사도 검색은 이미지 세분화(image segmentation) 작업 시에도 도움이 될 수 있습니다. 이미지 세분화는 이미지를 의미있는 영역이나 객체로 분할(partition)하는 것이 목표인 작업(task)입니다. 이미지의 조각(patch) 또는 수퍼픽셀(superpixel; 유사성을 띈 인접한 픽셀들을 모은 것, 참고)의 특징 벡터들을 비교하여 유사한 영역을 그룹화할 수 있습니다. 이렇게 하면 객체 간의 경계(boundary)를 식별하거나 유사한 시각적 특성(visual characteristic)을 가진 영역을 결정함으로써 객체 분할(object segmentation)이나 이미지 주석(image annotation) 같은 작업 시에 도움이 됩니다.\n",
      "Vector similarity search can assist in image segmentation tasks, where the goal is to partition an image into meaningful regions or objects. Similar regions can be grouped by comparing feature vectors of image patches or superpixels. This aids in identifying boundaries between objects or determining regions with similar visual characteristics, supporting tasks like object segmentation and image annotation.\n",
      "벡터 유사도 검색 요약 / Vector Similarity Search Summary\n",
      "벡터 유사도 검색은 고차원 공간에서 유사한 데이터 포인트들을 찾는데 사용하는 머신러닝의 핵심 기법(vital technique)입니다. 이는 추천 시스템이나 이미지 및 비디오 검색, 자연어 처리, 클러스터링 등에서 매우 중요합니다. 이 때 고차원 데이터를 다루거나 확장성, 거리 측정 기준 선택 방법 및 저장소 요구사항 등의 과제가 있습니다.\n",
      "Vector similarity search is a vital technique in machine learning used to find similar data points in high-dimensional spaces. It is crucial for recommendation systems, image and video search, NLP, clustering, and more. Challenges include high-dimensional data, scalability, choice of a distance metric, and storage requirements.\n",
      "이를 해결하기 위한 방법으로 차원 축소, 인덱싱 구조, 적응형 거리 측정 기준 및 신경망 해싱 등이 있습니다. 컴퓨터 비전 분야에서는 벡터 유사도 검색은 객체 탐지, 이미지 검색과 인식, 세분화에 사용하여 효율적인 검색과 빠른 감지, 정확한 인식을 가능하도록 합니다.\n",
      "머신러닝 어플리케이션을 향상시키고 사용자 경험을 개선하기 위해서는 벡터 유사도 검색을 완전히 익히는(mastering) 것이 필수적입니다.\n",
      "Solutions involve dimensionality reduction, indexing structures, adaptive distance metrics, and neural hashing. In computer vision, vector similarity search is used for object detection, image retrieval, recognition, and segmentation. It enables efficient retrieval, faster detection, and accurate recognition. Mastering vector similarity search is essential for enhancing machine learning applications and improving user experiences.\n",
      "주요 요점 정리 / Key Takeaways\n",
      "주의: 아래 내용은 Encord 서비스의 광고입니다. 파이토치 한국 사용자 모임은 Encord의 서비스와 어떠한 직/간접적인 관련이 없으며, 콘텐츠 번역을 허락해주신 것에 대한 감사의 의미로 아래 내용을 함께 번역하여 제공하고 있음을 알려드립니다.\n",
      "컴퓨터 비전 프로젝트의 품질, 속도, 정확성을 자동화하고 개선할 준비가 되셨나요?\n",
      "Ready to automate and improve the quality, speed, and accuracy of your computer vision projects?\n",
      "Encord 무료 평가판에 등록하세요: 세계 최고의 컴퓨터 비전팀에서 사용하는 컴퓨터 비전용 액티브 러닝 플랫폼입니다.\n",
      "Sign-up for an Encord Free Trial: The Active Learning Platform for Computer Vision, used by the world’s leading computer vision teams.\n",
      "AI가 지원하는 라벨링, 모델 학습 및 진단, 데이터셋의 오류와 편향을 찾고 수정할 수 있는, 모든 기능이 하나로 포함된 협업형 액티브 러닝 플랫폼을 사용하여 상용급 AI를 더 빠르게 얻을 수 있습니다. 지금 Encord를 무료로 사용해보세요.\n",
      "AI-assisted labeling, model training & diagnostics, find & fix dataset errors and biases, all in one collaborative active learning platform, to get to production AI faster. Try Encord for Free Today.\n",
      "Encord의 최신 소식을 받고 싶으신가요?\n",
      "Want to stay updated?\n",
      "좋은 정보 감사합니다\n",
      "최근에 vector를 활용한 silmilarity 유사도 체크를 진행하려고 했었는데 도움이 많이 되네요\n",
      "읽어주시고 댓글 남겨주셔서 감사합니다! ^^\n",
      "구현하시면서 궁금한 것이나 알게되신 것이 있으시면 나눠주시기를 부탁드립니다\n",
      "Discourse를 사용합니다. JavaScript가 활성화된 상태에서 가장 잘 보입니다.\n",
      "-----------------------------------\n",
      "{'title': '유클리드 기하학 완벽 가이드: 기본 공리부터 실생활 응용까지 : 네이버 블로그', 'source': 'https://m.blog.naver.com/femold/223296713608', 'score': 0.9083, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "블로그\n",
      "카테고리 이동\n",
      "마인드투자\n",
      "유클리드 기하학 완벽 가이드: 기본 공리부터 실생활 응용까지\n",
      "2023. 12. 18. 22:25\n",
      "유클리드 기하학 완벽가이드\n",
      "수학의 기본이 되는 '유클리드 기하학'에 대해 함께 이야기해보려 합니다. 고대 그리스 수학자 유클리드의 업적을 통해 우리의 일상과 밀접한 관련이 있는 이 흥미로운 분야를 탐구해봅시다. 우선, 간단한 질문과 답변을 통해 유클리드 기하학의 세계로 들어가 볼까요?\n",
      "유클리드관련 질문과 답\n",
      "Q: 유클리드는 누구인가요?\n",
      "A: 유클리드는 고대 그리스의 수학자로, '기하학의 아버지'로 불립니다. 그의 저서 \"원론\"은 수학사에서 가장 영향력 있는 책 중 하나입니다.\n",
      "​\n",
      "Q: 유클리드 기하학이란 무엇인가요?\n",
      "A: 유클리드 기하학은 평면과 공간에 대한 연구로, 점, 선, 면의 관계를 다룹니다. 이는 전통적인 기하학의 기초를 형성합니다.\n",
      "​\n",
      "Q: 유클리드 기하학의 기본 공리는 무엇인가요?\n",
      "A: 유클리드 기하학은 기본적인 가정들, 즉 '공리'와 '공준'에 기초합니다. 예를 들어, 한 점을 지나는 직선은 유일하다는 가정이 있습니다.\n",
      "​\n",
      "Q: 유클리드 기하학과 비유클리드 기하학의 차이점은 무엇인가요?\n",
      "A: 유클리드 기하학은 평면에서의 기하학을 다루는 반면, 비유클리드 기하학은 곡면에서의 기하학을 탐구합니다.\n",
      "​\n",
      "Q: 유클리드 기하학의 실생활 적용 예시는 무엇이 있나요?\n",
      "A: 건축학, 엔지니어링, 예술 등 다양한 분야에서 유클리드 기하학의 원리가 적용됩니다.\n",
      "유클리드와 그의 기하학 세계\n",
      "유클리드 초상화\n",
      "유클리드는 기하학뿐만 아니라 수학 전반에 걸쳐 중요한 역할을 했습니다. 그의 저서 \"원론\"은 기하학의 여러 개념과 정리를 체계적으로 정리한 책으로, 오늘날에도 그 가치가 인정받고 있습니다. 유클리드 기하학의 핵심은 '공리'라고 불리는 기본적인 가정에서 출발합니다. 이 공리들은 명확하고 간단하지만, 이를 통해 복잡한 기하학적 구조와 정리들을 도출할 수 있습니다.\n",
      "\"원론\" 책 표지\n",
      "유클리드 기하학의 중요한 개념 중 하나는 '점과 선'입니다. 점은 위치를 나타내지만 크기는 없는 것으로 정의되고, 선은 길이는 있지만 너비는 없는 것으로 정의됩니다.\n",
      "​\n",
      "이러한 기본적인 정의들을 바탕으로, 유클리드 기하학은 더 복잡한 형태와 구조를 탐구합니다. 예를 들어, 직선, 원, 삼각형과 같은 기본적인 도형들의 성질을 이해하고, 이를 활용하여 보다 복잡한 기하학적 문제를 해결할 수 있습니다.\n",
      "기본 도형과 그 속성들\n",
      "유클리드 기하학은 또한 증명과 논리의 중요성을 강조합니다. 그는 기하학적 명제와 정리들을 체계적으로 증명함으로써, 수학적 사고와 논리적 추론의 기초를 제공합니다. 이러한 접근 방식은 오늘날의 수학 교육과 연구에 큰 영향을 끼쳤습니다.\n",
      "유클리드 기하학의 증명 예시\n",
      "하지만 유클리드 기하학의 영향은 수학에만 국한되지 않습니다. 건축, 예술, 디자인과 같은 다양한 분야에서도 그의 기하학적 원리가 적용되고 있습니다. 예를 들어, 건축가들은 유클리드 기하학을 사용하여 건물의 구조를 설계하고, 예술가들은 도형과 비례를 이용하여 미적 감각을 표현합니다.\n",
      "유클리드 기하학이 적용된 건축 예시\n",
      "또한, 유클리드 기하학은 현대 과학과 기술의 발전에도 중요한 역할을 합니다. 예를 들어, 컴퓨터 그래픽스, 기계 설계, 우주 공학 등에서 기하학적 개념과 원리가 광범위하게 사용되고 있습니다.\n",
      "기술과 과학에서의 유클리드 기하학 적용\n",
      "유클리드와 그의 기하학은 수학과 과학, 심지어 우리 일상 생활에까지 깊숙이 스며들어 있습니다. 그의 기하학적 원리는 단순함 속에 깊은 통찰을 담고 있으며, 오늘날에도 여전히 우리에게 중요한 교훈을 제공합니다. 유클리드 기하학을 통해 우리는 논리적 사고와 창의적 문제 해결 능력을 키울 수 있으며, 더 넓은 세상을 이해하는 데 도움을 받을 수 있습니다.\n",
      "​\n",
      "유클리드와 관련된 공식\n",
      "유클리드 기하학의 핵심은 주로 기본적인 도형들의 성질과 그들 간의 관계에 관한 것입니다. 유클리드 기하학에서 다루는 몇 가지 중요한 공식들은 다음과 같습니다:\n",
      "유클리드와 관련된 공식\n",
      "​\n",
      "이러한 기본적인 공식들은 유클리드 기하학에서 출발점이며, 이를 기반으로 더 복잡한 기하학적 구조와 문제들을 해결할 수 있습니다. 유클리드 기하학은 공간과 형태를 이해하는 데 기본적인 도구로 사용됩니다.\n",
      "유클리드 기하학\n",
      "유클리드 기하학\n",
      "고대 그리스 수학자 유클리드에 의해 정립된 전통적인 기하학의 한 분야입니다. 이는 다른 수학 분야와 비교할 때 몇 가지 독특한 특징을 가지고 있으며, 여기서는 유클리드 기하학과 다른 관련 학문들을 비교하여 설명해보겠습니다.\n",
      "유클리드 기하학\n",
      "평면이나 공간에서 점, 선, 면의 관계를 다룹니다.\n",
      "기본적인 개념으로는 점, 선, 평면 등이 있으며, 이들을 기반으로 도형의 성질을 탐구합니다.\n",
      "유클리드의 \"원론\"에 의해 정리된 기본 공리와 정리들에 기반합니다.\n",
      "직선은 무한히 길며, 두 점을 이으면 단 하나의 직선을 만들 수 있다는 등의 공리를 포함합니다.\n",
      "비유클리드 기하학\n",
      "유클리드 기하학의 공리 중 하나 이상을 변경하거나 제거함으로써 발전한 기하학의 분야입니다.\n",
      "예를 들어, 곡면에서의 기하학을 다루는 리만 기하학은 평면이 아닌 곡면에서의 성질을 연구합니다.\n",
      "공간이 곡률을 가진다는 개념을 포함하며, 이는 일반 상대성 이론에서 중요한 역할을 합니다.\n",
      "해석 기하학\n",
      "데카르트가 도입한 기하학으로, 대수학과 기하학을 결합합니다.\n",
      "좌표계를 사용하여 기하학적 도형을 대수적 식으로 표현합니다.\n",
      "이를 통해 복잡한 기하학적 문제를 대수적 방법으로 해결할 수 있습니다.\n",
      "예를 들어, 원이나 타원 같은 곡선을 방정식으로 표현합니다.\n",
      "위상 기하학\n",
      "공간의 연속적인 변형에 대해 연구하는 기하학의 분야입니다.\n",
      "도형의 정확한 형태나 크기보다는, 그 도형이 가지는 연결성, 구멍의 수 등의 성질에 초점을 맞춥니다.\n",
      "\"고무 시트 기하학\"이라고도 불리며, 늘리거나 구부려도 변하지 않는 성질을 연구합니다.\n",
      "​\n",
      "각각의 기하학 분야는 독특한 관점과 방법론을 가지고 공간과 형태를 이해합니다. 유클리드 기하학은 가장 기본적이고 전통적인 형태의 기하학이며, 이후 발전한 다른 기하학 분야들은 유클리드 기하학의 한계를 넘어서 다양한 방식으로 공간을 탐구하고 있습니다.\n",
      "​\n",
      "유클리드의 생애와 배경\n",
      "유클리드 초상화\n",
      "유클리드는 기원전 3세기 경 고대 그리스의 알렉산드리아에서 활동한 수학자입니다.\n",
      "그의 생애에 대한 구체적인 정보는 많지 않지만, 그의 작업은 헬레니즘 시대의 학문적 발전을 대표합니다.\n",
      "\"원론\" (Elements)\n",
      "유클리드가 저술한 \"원론\"은 기하학의 기본 원리와 개념을 체계적으로 정리한 책입니다.\n",
      "이 책은 13권으로 구성되어 있으며, 기하학은 물론 수론과 무리수에 대한 내용도 포함하고 있습니다.\n",
      "유클리드 기하학의 기본 공리와 공준\n",
      "유클리드 기하학은 기본적인 가정(공리)에 기반을 두고 논리적으로 정리와 명제를 도출합니다.\n",
      "가장 유명한 공리로는 \"한 점을 지나는 직선은 유일하다\"는 것이 있습니다.\n",
      "유클리드 기하학의 중요한 정리들\n",
      "피타고라스 정리, 삼각형의 내각의 합이 180도라는 정리 등이 유클리드 기하학에서 중요한 역할을 합니다.\n",
      "이러한 정리들은 오늘날까지 기하학의 기본적인 구조를 이해하는 데 필수적입니다.\n",
      "수학과 과학에 미친 영향\n",
      "유클리드의 저작은 수학적 사고와 논리적 추론의 방법론을 확립하는 데 크게 기여했습니다.\n",
      "그의 작업은 중세 유럽과 이슬람 세계의 학자들에게 큰 영향을 미쳤으며, 르네상스 시대의 학문적 부흥에 중요한 역할을 했습니다.\n",
      "유클리드 기하학과 비유클리드 기하학\n",
      "유클리드 기하학은 평면 기하학의 특성을 다루는 반면, 비유클리드 기하학은 유클리드의 공리를 벗어난 공간을 탐구합니다.\n",
      "이러한 개념의 확장은 특히 일반 상대성 이론 등 현대 물리학에서 중요합니다.\n",
      "유클리드의 작업은 수학의 발전뿐만 아니라, 과학적 방법론과 논리적 사고의 발전에도 중요한 기여를 했습니다. 그의 이론과 원칙은 오늘날 수학 교육의 기초를 이루고 있으며, 다양한 학문 분야에서 참조되고 있습니다.\n",
      "​\n",
      "오늘은 우리 주변에서 쉽게 찾아볼 수 있는, 하지만 종종 무시되는 수학의 중요 개념인 '유클리드 거...\n",
      "blog.naver.com\n",
      "#유클리드 #기하학 #원론 #수학의아름다움 #공간의이해 #수학적사고 #과학과기술 #일상속기하학 #수학교육 #건축과예술\n",
      "​\n",
      "배움의 힘, 부의 시작: 마음을 단단히, 재산을 탄탄히.\n",
      "이 블로그\n",
      "일상생활 통계\n",
      "카테고리 글\n",
      "카테고리\n",
      "이 블로그\n",
      "일상생활 통계\n",
      "카테고리 글\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content)\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.document_compressors import (\n",
    "    DocumentCompressorPipeline,\n",
    "    EmbeddingsFilter,\n",
    ")\n",
    "from langchain.retrievers import (\n",
    "    ContextualCompressionRetriever,\n",
    "    TavilySearchAPIRetriever,\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=20)\n",
    "relevance_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.8)\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "\ttransformers=[splitter, relevance_filter]\n",
    ")\n",
    "base_tavily_retriever = TavilySearchAPIRetriever(\n",
    "\tk=6, include_raw_content=True, include_images=True\n",
    ")\n",
    "\n",
    "tavily_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor, base_retriever=base_tavily_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "확장성 / Scalability\n",
      "대규모 데이터셋을 검색하는데는 연산 비용과 시간이 많이 소요될 수 있습니다. 벡터 유사도 검색 알고리즘과 데이터 구조는 검색 공간을 효율적으로 정리(prune)하여, 거리 연산 횟수를 줄이고 유사한 항목들을 더 빠르게 검색할 수 있는 방법을 제공합니다.\n",
      "Searching through large datasets can be computationally expensive and time-consuming. Vector similarity search algorithms and data structures provide efficient ways to prune the search space, reducing the number of distance computations required and enabling faster retrieval of similar items.\n",
      "비정형 또는 반정형 데이터 / Unstructured or Semi-Structured Data\n",
      "벡터 유사도 검색은 이미지, 동영상, 텍스트 문서 또는 센서값 등과 같은 비정형 또는 반정형 데이터를 다룰 때 매우 중요합니다. 이러한 데이터 타입은 자연스럽게 벡토 또는 특징 벡터(feature vector)로 표현되며, 거리 값(metric)을 사용하면 유사도를 더 잘 파악할 수 있습니다.\n",
      "-----------------------------------\n",
      "{'title': 'Python 자연어 처리 백터의 유사도 (코사인, 유클리드, 자카드) : 네이버 블로그', 'source': 'https://m.blog.naver.com/yug311861/222965948490', 'score': 0.92868, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "블로그\n",
      "카테고리 이동\n",
      "꾹의 개발노트\n",
      "Python 자연어 처리 백터의 유사도 (코사인, 유클리드, 자카드)\n",
      "2022. 12. 26. 13:30\n",
      "Python 자연어 처리\n",
      "백터의 유사도 (코사인, 유클리드, 자카드)\n",
      "© cdr6934, 출처 Unsplash\n",
      "오랜만에 파이썬 자연어관련 블로그 포스팅을 합니다.\n",
      "포스팅에 앞 서 모두들 크리스마스는 재미있게 보내셨는지요.\n",
      "저는 아내와 재미있는 시간을 보냈습니다.\n",
      "내년 소원도 같이 빌고 즐겁게 시간을 보냈어요 슝슝\n",
      "벌써 12월의 마지막 주라니 말도 안돼 2022년 정말 즐거운 한 해 였는데 기분 좋은 일이 더 많은 한 해였습니다.\n",
      "오늘 하루도 회사에서 새로운 다짐을 하고 있어요!\n",
      "본격적으로 백터의 유사도란?\n",
      "우리는 이전 포스팅에서\n",
      "수치화 유사도에 대해서 배웠습니다 Bow TF-IDF Word2Vec이 대표적인 수치화 방법이 였습니다.\n",
      "오늘은 수치화된 결과를 백터 유사도를 활용한 방법론에 대해서 알아보는 시간이 될 것 같습니다.\n",
      "​\n",
      "간략하게 정리하면 수치화된 결과를 토대로 문서의 유사도를 분석하는 방법입니다!\n",
      "대표적은 코사인\n",
      "그 외 유클리드, 자카드가 있습니다!\n",
      "그럼 하나하나 체크해보겠습니다!\n",
      "Python 코사인 유사도\n",
      "문서의 단어나 문장을 수치화 한 갚을 토대로 구하는 방법이며\n",
      "Numpy - norm (놈)을 활용하여 간단하게 테스트가 가능합니다!\n",
      "공식은 위와 같다고 합니다!\n",
      "자연어 처리 코사인 유사도는 위와 같이 두 데이터를 비교하여 각도를 기준으로 -1 0 1이 표현됩니다.\n",
      "당연히 1에 가까울 수록 유사하다는 뜻이며 서로 직각을 이루면 0의 형태를 지닙니다!\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "Searching through large-scale datasets can be computationally expensive and time-consuming. As the dataset size grows, the search process becomes more challenging due to the increased number of vectors to compare. Efficient indexing structures and algorithms are necessary to handle the scalability issues associated with vector similarity search.\n",
      "거리 측정 기준 고르기 / Choice of Distance Metric\n",
      "벡터 유사도 검색에서는 적절한 거리 측정 지표(metric)를 선택하는 것이 중요합니다. 데이터의 특성과 적용 분야에 따라 서로 다른 거리 측정 지표는 서로 다른 결과를 야기(yield)할 수 있습니다. 원하는 유사도의 개념(notion of similarity)에 맞는 올바른 지표를 고르는 일은 결코 쉬운(non-trivial) 일이 아니므로 신중하게 고려해야 합니다.\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "Vector similarity search is employed in content filtering and search applications. By representing documents, articles, or web pages as vectors, similarity search enables efficient retrieval of relevant content based on their similarity to a query vector. This is useful in news recommendations, content filtering, or search engines.\n",
      "위에서 언급한 예시들은 다양한 분야(domain)에서 벡터 유사도 검색의 다양한 활용도(versatility)과 중요성(importance)를 보여주고 있습니다. 고차원 벡터와 유사도 기준(metric)의 기능을 활용(harness)함으로써, 벡터 유사도 검색으로 효율적인 정보 검색과 패턴 인식, 데이터 탐색 등을 용이하게 할 수 있습니다. 하지만, 현실 세계에서 벡터 유사도 검색을 적용할 때 마주칠 수 있는 문제들을 아는 것이 중요합니다.\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "The distribution and skewness of the data can impact the effectiveness of vector similarity search algorithms. Non-uniform data distributions or imbalanced data can lead to biased search results or suboptimal query performance. Handling data skewness and designing algorithms that are robust to varying data distributions pose challenges.\n",
      "결과의 설명 가능성 / Interpretability of Results\n",
      "벡터 유사도 검색은 주로 수학적 유사도를 측정한 값을 기반으로 유사한 벡터를 식별하는데 중점을 둡니다. 이는 많은 어플리케이션에서 효과적일 수 있지만, 결과를 설명하는 것(interpretability)은 어려울 수 있습니다. 두 벡터가 유사한 것으로 판단(consider)한 이유를 이해하고, 결과에서 의미있는 인사이트를 도출하려면 추가적인 후처리(post-processing)와 분야별 전문지식(domain-specific knowledge)이 필요할 수 있습니다.\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "데이터 포인트가 벡터로 표현되면 유사도 점수를 계산하여 두 벡터 간의 유사도를 정량화(quantify)합니다. 유사도 계산에 사용하는 일반적인 거리 기준(metric)에는 유클리드 거리(Euclidean distance), 멘하탄 거리(Manhattan distance), 코사인 유사도(cosine similarity) 등이 있습니다.\n",
      "Once the data points are represented as vectors, a similarity score is computed to quantify the similarity between two vectors. Common distance metrics used for similarity computation include Euclidean distance, Manhattan distance, and cosine similarity.\n",
      "유클리드 거리는 공간 내의 두 점간의 직선 거리를 측정하고, 맨하탄 거리는 각 차원의 차이의 절대값의 합을 계산하며, 코사인 유사도는 두 벡터 사이 각도의 코사인 값을 측정합니다. 거리 기준(metric)을 고르는 기준은 데이터의 특성과 사용 중인 어플리케이션에 따라 달라집니다.\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "벡터 유사도 검색의 사용 사례 / Use cases for Vector Similarity Search\n",
      "백터 유사도 검색은 다양한 분야(domain)에서 수많은 예제와 사용 사례가 있습니다. 여기에서는 몇 가지 대표적인 사례를 살펴보겠습니다:\n",
      "Vector similarity search has numerous examples and use cases across various domains. Here are some prominent examples:\n",
      "추천 시스템 / Recommendation Systems\n",
      "벡터 유사도 검색은 추천 시스템에서 중요한 역할을 합니다. 사용자와 항목(item)을 벡터로 표현함으로써, 유사한 사용자나 항목(item)들을 찾아내어 개인화된 추천을 가능하게 합니다. 예를 들어, 온라인 상거래(e-commerce) 분야에서는 사용자 선호도에 따라 제품을 추천하거나 협업 필터링(CF; collaborative filtering)으로 유사한 사용자를 찾는데 사용할 수 있습니다.\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "벡터 유사도 검색은 효율적인 콘텐츠 기반의 이미지 검색을 가능하게 합니다. 이미지를 고차원의 특징 벡터로 변환함으로써 시각적으로 유사한 콘텐츠가 있는 이미지들을 빠르게 식별할 수 있습니다. 이는 사용자가 이미지를 입력하고 데이터베이스에서 시각적으로 유사한 이미지를 검색하는 역방향 이미지 검색(reverse image search)과 같은 어플리케이션에서 특히 유용합니다. 이러한 기능은 이미지 검색 엔진, 이미지에 기반한 제품 추천, 구조화(organization)를 위한 이미지 클러스터링 등의 어플리케이션 등에서 사용할 수 있습니다.\n",
      "Vector similarity search allows for efficient content-based image retrieval. By converting images into high-dimensional feature vectors, images with similar visual content can be identified quickly. This is particularly useful in applications such as reverse image search, where users input an image and retrieve visually similar images from a database. It enables applications like image search engines, product recommendations based on images, and image clustering for organizations.\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "Vector similarity search plays a crucial role in recommendation systems. By representing users and items as vectors, similarity search helps find similar users or items, enabling personalized recommendations. For example, in e-commerce, it can be used to recommend products based on user preferences or to find similar users for collaborative filtering.\n",
      "이미지 및 비디오 검색 / Image and Video Search\n",
      "벡터 유사도 검색은 이미지 및 비디오 검색 어플리케이션에서 널리 사용합니다. 이미지나 비디오를 고차원의 특징 벡터(feature vector)로 표현함으로써, 시각적으로 유사한 콘텐츠를 찾는데 도움을 줍니다. 역방향 이미지 검색, 콘텐츠 기반 추천, 동영상 검색과 같은 작업들에 사용합니다.\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)\n",
      "What is Vector Similarity Search?\n",
      "Vector similarity search is a fundamental technique in machine learning, enabling efficient data retrieval and precise pattern recognition. It p\n",
      "벡터 유사도 검색은 무엇인가요? / What is Vector Similarity Search?\n",
      "Akruti Acharya • June 12, 2023 • 5 min read\n",
      "벡터 유사도 검색(Vector Similarity Search)은 머신러닝의 기본적인 기법으로, 효율적인 데이터 검색과 정확한 패턴 인식을 가능하게 합니다. 추천 시스템, 이미지 검색, 자연어 처리(NLP) 및 기타 어플리케이션 등에서 중추적인 역할을 수행하여 사용자 경험을 개선하고 데이터 기반의 의사결정을 주도합니다.\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "The selection of an appropriate distance metric is crucial in vector similarity search. Different distance metrics may yield varying results depending on the nature of the data and the application domain. Choosing the right metric that captures the desired notion of similarity is a non-trivial task and requires careful consideration.\n",
      "인덱싱 및 저장소 요구사항 파악하기 / Indexing and Storage Requirements\n",
      "고차원 공간에서 빠른 검색을 위해서는 효율적인 인덱싱 구조가 필수적입니다. 이러한 인덱스를 구축 및 유지하려면 스토리지 오버헤드가 발생할뿐만 아니라 연산 자원도 필요로 합니다. 인덱싱의 효율성과 스토리지의 요구 사항 사이의 균형을 맞추는 것(trade-off)은 벡터 유사도 검색에 있어서 어려운 과제입니다.\n",
      "-----------------------------------\n",
      "{'title': '[딥러닝 Nlp] 05. 벡터의 유사도(코사인, 유클리드, 자카드)', 'source': 'https://222ys.tistory.com/17', 'score': 0.93275, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "코딩빌런 솜지\n",
      "POWERED BY TISTORY\n",
      "[딥러닝 NLP] 05. 벡터의 유사도(코사인, 유클리드, 자카드)\n",
      "문서의 유사도 : 문서들 간에 동일하거나 비슷한 단어가 공통적으로 사용된 정도\n",
      "성능: 단어 표현 방법(DTM, Word2Vec 등) 과 유사도 기법(유클리드 거리, 코사인 유사도 등) 에 따라 달라짐\n",
      "# 05-01 코사인 유사도(Cosine Similarity)\n",
      "1. 코사인 유사도란?\n",
      "-두 벡터가 가리키는 방향이 유사한 정도를 코사인 각도로  구하는 기법\n",
      "-동일 = 1, 90° = 0, 180° = -1 이므로, -1 이상 1 이하이며  1에 가까울수록 유사\n",
      "-특징: 문서의 길이가 다른 상황에서 비교적 공정한 비교 가능\n",
      "Ex) 문서A, 문서B는 내용 비슷한데 길이는 2배 차이 / 문서A, 문서C는 다른 내용인데 길이는 비슷한 경우\n",
      "유클리드 거리는 길이의 영향 받아 A와 C의 유사도가 더 높게 나와버리지만,\n",
      "코사인 유사도는 내용(벡터방향)에 영향 받으므로 A와 B의 유사도가 더 높게 나옴\n",
      "2. 코사인 유사도 구하기\n",
      "-1단계: 문서를 벡터화 (DTM, TF-IDF 등)\n",
      "-2단계: 넘파이로 코사인 유사도 함수 구현해 계산\n",
      "3. TF-IDF와 코사인유사도로 영화추천시스템 구현\n",
      "-캐글 영화 데이터셋 링크: https://www.kaggle.com/rounakbanik/the-movies-dataset\n",
      "-1단계: TF-IDF 연산시 결측값 존재하면 에러 발생하므로 결측값 처리\n",
      "-2단계: overview 컬럼에 대한 TF-IDF 구하기\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "Vector similarity search is used in social network analysis to find similar individuals based on their social connections or behavior. By representing individuals as vectors and measuring their similarity, it assists in tasks such as finding influential users, detecting communities, or suggesting potential connections.\n",
      "콘텐츠 필터링 및 검색 / Content Filtering and Search\n",
      "벡터 유사도 검색은 콘텐츠 필터링 및 검색 어플리케이션에서 사용합니다. 유사도 검색은 문서, 기사 또는 웹 페이지를 벡터로 표현함으로써 질의 벡터(query vector)와의 유사도를 기반으로 관련 콘텐츠를 효율적으로 검색할 수 있게 해줍니다. 이는 뉴스 추천, 콘텐츠 필터링 및 검색엔진 등에서 유용합니다.\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "Vector similarity search is a vital technique in machine learning used to find similar data points in high-dimensional spaces. It is crucial for recommendation systems, image and video search, NLP, clustering, and more. Challenges include high-dimensional data, scalability, choice of a distance metric, and storage requirements.\n",
      "이를 해결하기 위한 방법으로 차원 축소, 인덱싱 구조, 적응형 거리 측정 기준 및 신경망 해싱 등이 있습니다. 컴퓨터 비전 분야에서는 벡터 유사도 검색은 객체 탐지, 이미지 검색과 인식, 세분화에 사용하여 효율적인 검색과 빠른 감지, 정확한 인식을 가능하도록 합니다.\n",
      "머신러닝 어플리케이션을 향상시키고 사용자 경험을 개선하기 위해서는 벡터 유사도 검색을 완전히 익히는(mastering) 것이 필수적입니다.\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "이미지 인식 / Image Recognition\n",
      "벡터 유사도 검색은 입력 이미지의 특징 벡터를 이미 갖고있는(known) 이미지들의 인덱싱된 벡터들과 비교하여 이미지 인식 작업을 용이하게 합니다. 이러한 기능은 이미지 분류(image classification) 및 이미지 유사도 순위 지정(image similarity ranking) 등에 사용합니다.\n",
      "Vector similarity search facilitates image recognition tasks by comparing feature vectors of input images with indexed vectors of known images. This aids in tasks like image classification and image similarity ranking.\n",
      "예를 들어, 이미지 인식 시스템에서는 질의 이미지(query image)의 특징 벡터를 알려진 분류(class) 또는 이미 인덱싱된 이미지들의 벡터와 비교하여 가장 유사한 분류(class) 또는 이미지를 찾습니다. 이러한 접근 방식은 얼굴 인식이나 장면 인식, 이미지 분류(image categorization)과 같은 어플리케이션에 사용됩니다.\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "Traditional search methods struggle to handle this curse of dimensionality efficiently. Using embeddings can reduce these dimensions to 1,024, which is significantly lower. This reduction offers advantages such as decreased storage space, faster computations, and elimination of irrelevant information while preserving important features.\n",
      "키워드 기반 검색의 비효율성 / Ineffective keyword-based search\n",
      "키워드 기반 검색이나 일치 검색(exact match)과 같은 기존의 검색 방법은 명시적 키워드가 아닌, 다차원적인 특징(characteristic)을 기반으로 유사도(similarity)를 찾는 시나리오에는 적합하지 않습니다.\n",
      "Traditional search methods, such as keyword-based search or exact matching, are not suitable for scenarios where similarity is based on multi-dimensional characteristics rather than explicit keywords.\n",
      "확장성 / Scalability\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "Efficient indexing structures are essential to support fast search operations in high-dimensional spaces. Constructing and maintaining these indexes can incur storage overhead and necessitate computational resources. Balancing the trade-off between indexing efficiency and storage requirements is a challenge in vector similarity search.\n",
      "정확도와 효율성 사이의 균형 맞추기 / The trade-off between Accuracy and Efficiency\n",
      "많은 근사 최인접 이웃(ANN; Approximate Nearest Neighbor) 알고리즘은 더 빠른 검색 성능을 위해 정확도를 어느 정도 희생합니다. 검색의 정확도와 계산 효율성 사이의 적절한 균형을 맞추는 것은 매우 중요하며, 이는 어플리케이션의 요구사항과 제약 조건에 따라 달라집니다.\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "Vector similarity search is crucial when dealing with unstructured or semi-structured data types, such as images, videos, text documents, or sensor readings. These data types are naturally represented as vectors or feature vectors, and their similarity is better captured using distance metrics.\n",
      "정형 데이터와 비정형 데이터의 차이를 보여주는 그림. 출처 / Illustration showing the difference between structured and unstructured data. Source\n",
      "벡터 유사도 검색은 이러한 문제들을 해결함으로써 추천 시스템, 콘텐츠 기반 검색, 이상 징후 감지(anomaly detection), 클러스터링 등과 같은 다양한 머신러닝 및 데이터 분석 작업의 효율성과 효과를 향상시킵니다.\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "신경망이 학습되면, 이를 사용하여 데이터셋 내의 벡터들을 가지고 바이너리 코드를 생성합니다. 각각의 벡터는 바이너리 코드로 인코딩(encode)되며, 이 때 바이너리 코드의 각 비트들은 특정한 특징(feature) 또는 벡터의 특징(characteristic)을 표현합니다.\n",
      "Once the neural network is trained, it is used to generate binary codes for the vectors in the dataset. Each vector is encoded as a binary code, where each bit represents a specific feature or characteristic of the vector.\n",
      "유사도 검색 단계에서는 원래의 고차원 벡터 대신 바이너리 코드들을 비교합니다. 이 때, 바이너리 코드들 간의 비교는 고차원 벡터들 간의 비교에 비해서 (유사도를) 계산하는 비용이 저렴하기 때문에 효율적으로 검색을 할 수 있습니다.\n",
      "During the similarity search phase, the binary codes are compared instead of the original high-dimensional vectors. This enables efficient search operations, as comparing binary codes is computationally inexpensive compared to comparing high-dimensional vectors.\n",
      "-----------------------------------\n",
      "{'title': '벡터 유사도 검색이 무엇인가요? (What is Vector Similarity Search?)', 'source': 'https://discuss.pytorch.kr/t/what-is-vector-similarity-search/2475', 'score': 0.97289, 'images': ['https://i.ytimg.com/vi/-4qequgou6E/maxresdefault.jpg', 'https://i.ytimg.com/vi/O-Ns2ADCsPg/maxresdefault.jpg', 'https://i.ytimg.com/vi/aXosYpzK90k/maxresdefault.jpg', 'https://s3.orbi.kr/data/file/united2/ce2fc18d9f1e424c969019d7dce26522.png', 'https://w1.pngwing.com/pngs/322/710/png-transparent-unit-circle-trigonometric-functions-law-of-cosines-trigonometry-law-of-sines-angle-inverse-trigonometric-functions-mathematics.png']}\n",
      "이번 글에서는 머신러닝 분야에서의 벡터 유사도 검색의 중요성과 AI 임베딩이 이를 개선하는데 어떻게 도움이 되는지에 대해 설명합니다. 다음 내용들을 다룹니다:\n",
      "In this blog, we’ll discuss the importance of vector similarity search in machine learning and how AI embeddings can help improve it. We will cover the:\n",
      "벡터 유사도 검색이 해결하는 문제는 무엇인가요? / What Problem is Vector Similarity Search Solving?\n",
      "벡터 유사도 검색은 대규모 데이터셋, 특히 고차원 공간에서 유사한 항목이나 데이터 포인트를 효율적으로 검색해야 하는 문제를 해결합니다. 다음은 벡터 유사도 검색으로 해결할 수 있는 몇 가지 문제들입니다:\n",
      "Vector similarity search addresses the challenge of efficiently searching for similar items or data points in large datasets, particularly in high-dimensional spaces. Here are some of the problems vector similarity search addresses:\n",
      "차원의 저주 / Curse of Dimensionality\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "docs = tavily_retriever.get_relevant_documents(query)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content)\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_TEMPLATE = \"\"\"\\\n",
    "You are an expert researcher and writer, tasked with answering any question.\n",
    "\n",
    "Generate a comprehensive and informative, yet concise answer of 250 words or less for the \\\n",
    "given question based solely on the provided search results (URL and content). You must \\\n",
    "only use information from the provided search results. Use an unbiased and \\\n",
    "journalistic tone. Combine search results together into a coherent answer. Do not \\\n",
    "repeat text. Cite search results using [${{number}}] notation. Only cite the most \\\n",
    "relevant results that answer the question accurately. Place these citations at the end \\\n",
    "of the sentence or paragraph that reference them - do not put them all at the end. If \\\n",
    "different results refer to different entities within the same name, write separate \\\n",
    "answers for each entity. If you want to cite multiple results for the same sentence, \\\n",
    "format it as `[${{number1}}] [${{number2}}]`. However, you should NEVER do this with the \\\n",
    "same number - if you want to cite `number1` multiple times for a sentence, only do \\\n",
    "`[${{number1}}]` not `[${{number1}}] [${{number1}}]`\n",
    "\n",
    "You should use bullet points in your answer for readability. Put citations where they apply \\\n",
    "rather than putting them all at the end.\n",
    "\n",
    "If there is nothing in the context relevant to the question at hand, just say \"Hmm, \\\n",
    "I'm not sure.\" Don't try to make up an answer.\n",
    "\n",
    "Anything between the following `context` html blocks is retrieved from a knowledge \\\n",
    "bank, not part of the conversation with the user.\n",
    "\n",
    "<context>\n",
    "    {context}\n",
    "<context/>\n",
    "\n",
    "REMEMBER: If there is no relevant information within the context, just say \"Hmm, I'm \\\n",
    "not sure.\" Don't try to make up an answer. Anything between the preceding 'context' \\\n",
    "html blocks is retrieved from a knowledge bank, not part of the conversation with the \\\n",
    "user. The current date is {current_date}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPHRASE_TEMPLATE = \"\"\"\\\n",
    "Given the following conversation and a follow up question, rephrase the follow up \\\n",
    "question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone Question:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import List, Optional, Sequence, Tuple, Union\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.schema.language_model import BaseLanguageModel\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.retriever import BaseRetriever\n",
    "from langchain.schema.runnable import (\n",
    "    ConfigurableField,\n",
    "    Runnable,\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableMap,\n",
    ")\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.schema.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatRequest(BaseModel):\n",
    "    question: str\n",
    "    chat_history: List[Tuple[str, str]] = Field(\n",
    "        ...,\n",
    "        extra={\"widget\": {\"type\": \"chat\", \"input\": \"question\", \"output\": \"answer\"}},\n",
    "    )\n",
    "\n",
    "\n",
    "def serialize_history(request: ChatRequest):\n",
    "    chat_history = request.get(\"chat_history\", [])\n",
    "    converted_chat_history = []\n",
    "    for message in chat_history:\n",
    "        if message[0] == \"human\":\n",
    "            converted_chat_history.append(HumanMessage(content=message[1]))\n",
    "        elif message[0] == \"ai\":\n",
    "            converted_chat_history.append(AIMessage(content=message[1]))\n",
    "    return converted_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever_chain(\n",
    "    llm: BaseLanguageModel, retriever: BaseRetriever\n",
    ") -> Runnable:\n",
    "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(REPHRASE_TEMPLATE)\n",
    "    condense_question_chain = (\n",
    "        CONDENSE_QUESTION_PROMPT | llm | StrOutputParser()\n",
    "    ).with_config(\n",
    "        run_name=\"CondenseQuestion\",\n",
    "    )\n",
    "    conversation_chain = condense_question_chain | retriever\n",
    "    return RunnableBranch(\n",
    "        (\n",
    "            RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "                run_name=\"HasChatHistoryCheck\"\n",
    "            ),\n",
    "            conversation_chain.with_config(run_name=\"RetrievalChainWithHistory\"),\n",
    "        ),\n",
    "        (\n",
    "            RunnableLambda(itemgetter(\"question\")).with_config(\n",
    "                run_name=\"Itemgetter:question\"\n",
    "            )\n",
    "            | retriever\n",
    "        ).with_config(run_name=\"RetrievalChainWithNoHistory\"),\n",
    "    ).with_config(run_name=\"RouteDependingOnChatHistory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: Sequence[Document]) -> str:\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_string = f\"<doc id='{i}'>{doc.page_content}</doc>\"\n",
    "        formatted_docs.append(doc_string)\n",
    "    return \"\\n\".join(formatted_docs)\n",
    "\n",
    "def create_chain(\n",
    "    llm: BaseLanguageModel,\n",
    "    retriever: BaseRetriever,\n",
    ") -> Runnable:\n",
    "    retriever_chain = create_retriever_chain(llm, retriever) | RunnableLambda(\n",
    "        format_docs\n",
    "    ).with_config(run_name=\"FormatDocumentChunks\")\n",
    "    _context = RunnableMap(\n",
    "        {\n",
    "            \"context\": retriever_chain.with_config(run_name=\"RetrievalChain\"),\n",
    "            \"question\": RunnableLambda(itemgetter(\"question\")).with_config(\n",
    "                run_name=\"Itemgetter:question\"\n",
    "            ),\n",
    "            \"chat_history\": RunnableLambda(itemgetter(\"chat_history\")).with_config(\n",
    "                run_name=\"Itemgetter:chat_history\"\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", RESPONSE_TEMPLATE),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    ).partial(current_date=datetime.now().isoformat())\n",
    "\n",
    "    response_synthesizer = (prompt | llm | StrOutputParser()).with_config(\n",
    "        run_name=\"GenerateResponse\",\n",
    "    )\n",
    "    return (\n",
    "        {\n",
    "            \"question\": RunnableLambda(itemgetter(\"question\")).with_config(\n",
    "                run_name=\"Itemgetter:question\"\n",
    "            ),\n",
    "            \"chat_history\": RunnableLambda(serialize_history).with_config(\n",
    "                run_name=\"SerializeHistory\"\n",
    "            ),\n",
    "        }\n",
    "        | _context\n",
    "        | response_synthesizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatAnthropic, ChatOpenAI, ChatVertexAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gpt-3.5-turbo-16k\",\n",
    "#     # model=\"gpt-4\",\n",
    "#     streaming=True,\n",
    "#     temperature=0.1,\n",
    "# )\n",
    "# retriever = tavily_retriever\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     base_url=\"http://localhost:1234/v1\",\n",
    "#     temperature=0,\n",
    "#     api_key=\"lm-studio\",\n",
    "#     streaming=True,\n",
    "# )\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    "    # model=\"gpt-4\",\n",
    "    streaming=True,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "chain = create_chain(llm, tavily_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"tanstack/react-query에서 suspense 사용하는 방법을 알려줘\"\n",
    "\n",
    "result = chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "React Query에서 suspense를 사용하는 방법은 다음과 같습니다:\n",
      "\n",
      "1. React Query의 Suspense 모드를 활성화하려면 전역 또는 쿼리 수준의 설정에서 suspense 옵션을 true로 설정해야 합니다. [1]\n",
      "   - 전역 설정: `setQueryClientOptions({ suspense: true })`\n",
      "   - 쿼리 수준 설정: `useQuery(key, queryFn, { suspense: true })`\n",
      "\n",
      "2. Suspense 모드를 사용하면 쿼리 컴포넌트가 마운트되기 전에 데이터를 미리 로드할 수 있습니다. 이를 위해 라우팅 콜백이나 사용자 상호작용 이벤트에서 Prefetching을 구현하는 것이 좋습니다. [0]\n",
      "\n",
      "3. Suspense 모드에서는 상태 및 오류 객체 대신 React.Suspense 컴포넌트를 사용하여 상태 및 오류를 처리합니다. 오류 처리를 위해 React 에러 경계를 사용할 수도 있습니다. [3]\n",
      "\n",
      "4. Suspense 모드에서는 쿼리 오류가 발생하면 오류가 컴포넌트의 다음 렌더링에서 발생하고 가장 가까운 오류 경계로 전파됩니다. 이 동작을 비활성화하려면 useErrorBoundary 옵션을 false로 설정하면 됩니다. 오류가 전혀 발생하지 않도록 하려면 throwOnError 옵션을 false로 설정할 수도 있습니다. [2]\n",
      "\n",
      "5. Suspense 모드에서는 쿼리 컴포넌트의 마운트 시점에 쿼리를 가져오고 일시 중단할 수 있습니다. 이는 Fetch-on-render 솔루션으로 작동하며 추가 구성 없이 사용할 수 있습니다. [0]\n",
      "\n",
      "더 자세한 내용은 React Query의 공식 문서와 커뮤니티 리소스를 참조하시기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langserve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
